= Microsoft Sentinel Questions

== What is Microsoft Sentinel, and what is its primary purpose in a security operations environment?

Microsoft Sentinel is Microsoft's cloud-native Security Information and Event Management (SIEM) and Security Orchestration, Automation, and Response (SOAR) solution. It provides intelligent security analytics and threat intelligence across the enterprise.

*Primary purposes*:

* *Centralized security monitoring*:

** *Log aggregation* - collects data from entire organization (Azure, on-premises, multi-cloud), ingests logs from 100+ data sources (Azure services, Microsoft 365, third-party solutions, custom sources).
** *Unified visibility* - single pane of glass for security posture, correlates events across all environments, eliminates security blind spots.
** *Scale* - cloud-native architecture handles massive log volumes (petabytes), elastic scaling based on ingestion needs.

* *Threat detection*:
** *Analytics rules* - built-in detection rules from Microsoft security research, custom detection logic using Kusto Query Language (KQL), machine learning behavioral analytics.
** *Threat intelligence* - integration with Microsoft Threat Intelligence, custom threat feeds (STIX/TAXII), automated indicator matching.
** *Advanced detection* - User and Entity Behavior Analytics (UEBA), anomaly detection identifying unusual patterns, fusion detection correlating multiple weak signals.

* *Investigation and response*:
** *Incident management* - automated incident creation from alerts, case management with assignment and tracking, investigation graphs showing attack scope.
** *Hunting* - proactive threat hunting with KQL, hunting queries from Microsoft and community, notebooks for complex investigations.
** *Automation* - playbooks using Azure Logic Apps, automated response to common scenarios, integration with ticketing and communication systems.

*Key capabilities*:

* *Data connectors*: *Microsoft services* - Azure Active Directory, Microsoft 365 Defender, Azure services (NSG flow logs, Key Vault, etc.). *Third-party integrations* - Palo Alto, Check Point, AWS, GCP, Syslog/CEF sources. *Custom connectors* - REST API ingestion, custom parsers for proprietary formats.
* *Analytics*: *Scheduled queries* - run KQL queries on schedule detecting patterns, threshold-based alerting, correlation across multiple data sources. *Fusion* - ML-powered multi-stage attack detection, correlates low-fidelity signals into high-confidence incidents. *Anomaly rules* - baseline normal behavior, alert on statistical deviations.

*From security engineering perspective*:

* *Architecture benefits*:

** *Cloud-native* - no infrastructure to maintain, automatic updates and new features, global threat intelligence built-in.
** *Integration* - deep integration with Azure security stack, extends to multi-cloud and on-premises, API-driven for custom integrations.
** *Cost model* - pay-per-GB ingested (with commitment discounts), separate compute costs for analytics, predictable scaling costs.

* *Security use cases*:
** *Security monitoring* - monitor authentication failures, privilege escalations, malware detection, data exfiltration attempts.
** *Compliance* - centralized audit logs for compliance, retention policies meeting regulatory requirements, compliance workbooks (PCI-DSS, HIPAA, etc.).
** *Incident response* - automated triage reducing MTTR, playbooks for consistent response, threat hunting accelerating investigations.
** *Threat hunting* - proactive searches for IOCs, hunting for advanced persistent threats (APTs), behavioral analysis identifying insider threats.

*Example deployment scenario*: Organization with Azure workloads, Microsoft 365, and AWS infrastructure would: ingest Azure AD sign-in logs and audit logs, collect Microsoft 365 Defender alerts, forward AWS CloudTrail to Sentinel, enable NSG flow logs and Azure Firewall logs, deploy analytics rules detecting common attacks, create playbooks for automated response, configure UEBA for anomaly detection, and set up hunting queries for proactive defense.

Sentinel is comprehensive security operations platform enabling detection, investigation, and response at cloud scale - essential for organizations needing visibility across hybrid and multi-cloud environments.

== How does Microsoft Sentinel differ from traditional SIEM solutions, and what are the advantages of a cloud-native SIEM?

*Traditional SIEM challenges*:

* *Infrastructure overhead*:
** *Hardware/VM management* - deploy and maintain SIEM servers, storage arrays for log retention, networking and load balancing.
** *Capacity planning* - predict log volumes months in advance, overprovisioning to handle peaks, costly upgrades when scaling needed.
** *Maintenance burden* - patching and updates requiring downtime, software version management, database optimization and tuning.

* *Scalability limitations*:
** *Fixed capacity* - hardware limits ingestion rate, storage limits retention period, adding capacity requires procurement and deployment.
** *Performance degradation* - searches slow as data grows, need to archive old data reducing visibility, complex queries impact system performance.

*Cost structure*:

* *High upfront CapEx* - expensive hardware and software licenses, staff to deploy and maintain, redundancy for high availability.
* *Predictable but rigid* - costs fixed regardless of usage, can't scale down during low periods, difficult to justify for growing organizations.

*Microsoft Sentinel advantages*:

* *Cloud-native architecture*:
** *Elastic scalability*:
*** *Unlimited ingestion* - handles terabytes per day effortlessly, automatic scaling during peaks (incident response, attack), no performance degradation with data growth.
*** *Flexible retention* - standard retention in Log Analytics (30-730 days), archive to Azure Storage for long-term (years), query archived data when needed.
*** *Global infrastructure* - leverages Azure global datacenters, high availability built-in, disaster recovery automatic.

* *Pay-per-use model*:
** *No upfront costs* - no hardware to purchase, no deployment costs, start small and grow organically.
** *Cost optimization* - pay only for data ingested, commitment tiers for predictable workloads (100GB/day, 500GB/day), separate query costs allowing cost control.
** *Example*: 100GB/day = ~$3000/month (with commitment), much lower than traditional SIEM TCO.

* *Operational efficiency*:
** *Zero maintenance* - Microsoft handles all updates, new features automatically available, no downtime for maintenance.
** *No tuning* - automatic performance optimization, no database administration, no capacity planning.

* *Rapid deployment*:
** *Hours not months* - enable Sentinel in existing Azure tenant, connect data sources with pre-built connectors, start detecting threats same day.
** *Pre-built content* - 200+ out-of-box analytics rules, threat intelligence feeds included, workbooks for common scenarios.

*Advanced capabilities*:

* *Built-in AI/ML*: *Fusion detection* - multi-stage attack detection using ML, reduces false positives through correlation, identifies attack campaigns automatically. *UEBA* - behavioral baselines for users and entities, anomaly detection without manual rules, peer group analysis. *These capabilities extremely expensive in traditional SIEMs* or require add-on products.
* *Integration ecosystem*: *Microsoft ecosystem* - native integration with Azure AD, Microsoft 365, Azure services, unified security posture. *Third-party* - 100+ native connectors, REST API for custom integrations, community-contributed connectors.

*Comparison table*:

|===
| Aspect | Traditional SIEM | Microsoft Sentinel

| Deployment
| Weeks/months
| Hours/days

| Infrastructure
| Manage yourself
| Microsoft-managed

| Scaling
| Manual, hardware
| Automatic, elastic

| Cost model
| CapEx + OpEx
| OpEx only

| Updates
| Manual, downtime
| Automatic, no downtime

| Threat intel
| Purchase separately
| Included

| ML/Analytics
| Add-on products
| Built-in

| Multi-cloud
| Complex
| Native support

| Query language
| Proprietary
| KQL (widely used)
|===

*When Sentinel makes sense*:

* *Cloud-first organizations* - Azure or multi-cloud workloads, growing security teams, variable workloads needing elastic scaling.
* *Limited security staff* - small teams can't maintain traditional SIEM, need automation to scale, want to focus on threats not infrastructure.
* *Rapid deployment needs* - mergers/acquisitions requiring quick integration, new subsidiaries needing security monitoring, incident response requiring temporary capacity increase.

*When traditional SIEM might fit*:

* *Fully on-premises* - no cloud presence, regulations preventing cloud use.
* *Extremely high volumes* - hundreds of TB/day where Sentinel costs prohibitive (though rare), dedicated team to maintain SIEM.
* *Existing investment* - large sunk cost in current SIEM, contracts with remaining term, highly customized workflows.

*Hybrid approach*: Many organizations run both: Sentinel for cloud workloads and new deployments, traditional SIEM for legacy on-premises, gradual migration to Sentinel over time, bi-directional integration feeding data both ways.

From security engineering perspective, Sentinel represents paradigm shift from "security infrastructure management" to "security operations" - allowing teams to focus on detecting and responding to threats rather than maintaining systems.

== What are the key components of Microsoft Sentinel architecture?

Microsoft Sentinel architecture consists of several integrated components working together to provide comprehensive security operations.

*Core components*:

*1. Data Connectors*: Interface between data sources and Sentinel ingesting logs and alerts.

*Built-in connectors*:

* *Microsoft services* - Azure Active Directory (sign-ins, audit logs, identity protection), Microsoft 365 Defender (Defender for Endpoint, Defender for Office 365, Defender for Identity, Defender for Cloud Apps), Azure services (Activity logs, NSG flow logs, Key Vault, Firewall, Storage).
* *Security solutions* - Microsoft Defender for Cloud, Azure Web Application Firewall, Azure DDoS Protection.
* *Third-party* - Palo Alto Networks, Check Point, Cisco, AWS CloudTrail, GCP audit logs, Syslog/CEF (Common Event Format).

*Data connector types*:

* *Service-to-service* - native Azure integration via diagnostic settings, real-time streaming, no agent required.
* *Agent-based* - Log Analytics agent (MMA/AMA) for VMs, Syslog forwarder for Linux, Windows Event Forwarding.
* *API-based* - REST API connectors for SaaS applications, custom API connectors via Logic Apps, polling or webhook-based.

*Example connector configuration*:

[,kusto]
----
// Enable Azure AD connector
// Portal: Sentinel > Data connectors > Azure Active Directory
// Select: Sign-in logs, Audit logs, Identity Protection

// Results in tables:
SigninLogs
| where TimeGenerated > ago(1h)
| where ResultType != 0  // Failed sign-ins
| project TimeGenerated, UserPrincipalName, ResultType, ResultDescription

AuditLogs
| where TimeGenerated > ago(1h)
| where OperationName == "Add member to role"
| project TimeGenerated, InitiatedBy, TargetResources
----

*2. Log Analytics Workspace*: Underlying data store and query engine.

*Data storage*:

* *Tables* - each data source maps to table (SigninLogs, SecurityEvent, Syslog), custom tables for parsed data.
* *Retention* - configurable per table (30-730 days default), archive tier for long-term storage (years), restore from archive for investigations.
* *Partitioning* - automatic time-based partitioning, optimized for time-range queries.

*Query engine*: *Kusto Query Language (KQL)* - powerful query language for log analysis, similar to SQL but optimized for log data, supports complex analytics and aggregations.

*Example KQL query*:

[,kusto]
----
// Hunt for credential stuffing attack
SigninLogs
| where TimeGenerated > ago(24h)
| where ResultType != 0  // Failed
| summarize
    FailedAttempts = count(),
    UniqueIPs = dcount(IPAddress),
    UniqueUsers = dcount(UserPrincipalName)
    by IPAddress
| where FailedAttempts > 100 and UniqueUsers > 10
| order by FailedAttempts desc
----

*3. Analytics Rules*: Detection logic creating alerts and incidents.

*Rule types*:

* *Scheduled* - KQL queries running on schedule, most common type, flexible detection logic.
* *Microsoft security* - alerts from Microsoft security products (Defender, Cloud App Security), automatic incident creation.
* *Fusion* - ML-powered correlation of multiple signals, detects multi-stage attacks.
* *Anomaly* - ML-based behavioral anomaly detection, self-learning baselines.

*Analytics rule structure*:

[,kusto]
----
// Example: Detect suspicious Azure KeyVault access
AzureDiagnostics
| where ResourceType == "VAULTS"
| where OperationName == "SecretGet"
| where TimeGenerated > ago(1h)
| summarize
    SecretAccesses = count(),
    UniqueSecrets = dcount(id_s)
    by CallerIPAddress, identity_claim_upn_s
| where SecretAccesses > 50  // Threshold
| project
    TimeGenerated = now(),
    IPAddress = CallerIPAddress,
    User = identity_claim_upn_s,
    AccessCount = SecretAccesses,
    UniqueSecretsAccessed = UniqueSecrets
----

*Rule configuration*:

* *Scheduling* - query interval (every 5 min to 14 days), lookback period (how far back to search).
* *Threshold* - results threshold triggering alert, suppression to prevent alert floods.
* *Entity mapping* - map query results to entities (user, IP, host), enables investigation graph.
* *Tactics and techniques* - MITRE ATT&CK framework mapping, aids in understanding attack progression.

*4. Incidents*: Aggregation of related alerts into actionable cases.

*Incident creation*:

* *Grouping* - related alerts grouped into single incident, reduces alert fatigue, provides context.
* *Configuration* - group by entity, time proximity, alert name, custom logic.

*Incident properties*:

* *Severity* - Informational, Low, Medium, High, calculated from alert severity.
* *Status* - New, Active, Closed, used for case management.
* *Owner* - assigned analyst, facilitates accountability.
* *Comments* - investigation notes, collaboration.

*5. Workbooks*: Interactive dashboards for visualization and analysis.

*Built-in workbooks*:

* *Azure AD* - sign-in analysis, risky users, conditional access.
* *Microsoft 365* - email threats, Teams activity, SharePoint access.
* *Azure Activity* - resource modifications, role assignments, policy changes.
* *Threat Intelligence* - IOC matching, threat actor mapping.

*Custom workbooks*:

* *KQL-based* - any KQL query can be visualized, supports tables, charts, maps, timelines.
* *Interactive* - parameters for filtering, drill-down capabilities.

*6. Hunting*: Proactive threat hunting queries and notebooks.

*Hunting queries*:

* *Built-in* - curated queries from Microsoft security research, organized by MITRE ATT&CK tactics.
* *Custom* - save and share custom hunting queries, bookmarkable for future use.

*Notebooks*:

* *Jupyter notebooks* - complex multi-step investigations, Python code for advanced analysis, machine learning for pattern detection.
* *Integration* - query Sentinel data, external threat intelligence, visualization libraries.

*7. Automation (Playbooks)*: Automated response using Azure Logic Apps.

*Playbook triggers*:

* *Incident trigger* - run when incident created or updated, access incident properties (severity, entities).
* *Alert trigger* - run on specific alert, more granular than incident.

*Common playbook actions*:

* *Enrichment* - query threat intelligence, geolocate IP addresses, resolve DNS names.
* *Notification* - email security team, post to Teams/Slack, create ServiceNow ticket.
* *Response* - block IP in firewall, disable user account, isolate device in Defender.

*Example playbook flow*:

[,bash]
----
Incident created (High severity, contains IP entity)
â†’ Get IP reputation from VirusTotal
â†’ If malicious:
  â†’ Block IP in Azure Firewall
  â†’ Post alert to Teams channel
  â†’ Add comment to incident
â†’ Else:
  â†’ Add comment with reputation info
----

*8. Threat Intelligence*: Integration of threat indicators.

*Sources*:

* *Microsoft Threat Intelligence* - built-in feed from Microsoft, updated continuously.
* *Custom feeds* - STIX/TAXII feeds, CSV file upload, API integration.
* *ThreatConnect, Anomali*, etc.

*Matching*:

* *Automatic* - indicators matched against all ingested data, alerts created on matches.
* *Manual* - query ThreatIntelligenceIndicator table for hunting.

*9. Watchlists*: Reference data for enrichment and detection.

*Use cases*:

* *VIP users* - executives, administrators requiring special monitoring, heightened alerting.
* *Known good* - approved IP ranges, service accounts, expected behaviors.
* *Asset inventory* - critical servers, sensitive data stores, compliance-scoped systems.

*Example watchlist usage*:

[,kusto]
----
// Check if failed sign-in from VIP user
SigninLogs
| where ResultType != 0
| join kind=inner (
    _GetWatchlist('VIP-Users')
    | project UserPrincipalName
) on UserPrincipalName
| project TimeGenerated, UserPrincipalName, IPAddress, ResultDescription
----

*Architecture diagram flow*:

[,bash]
----
Data Sources â†’ Data Connectors â†’ Log Analytics Workspace
                                        â†“
                            â† Analytics Rules query data
                                        â†“
                                  Create Alerts
                                        â†“
                              Group into Incidents
                                        â†“
                         â† Analysts investigate (Workbooks, Hunting)
                                        â†“
                            Trigger Playbooks (Automation)
                                        â†“
                            Response Actions (Block, Disable, etc.)
----

*Data flow and security*:

* *Ingestion* - data encrypted in transit (TLS), authenticated connections, rate limiting and throttling.
* *Storage* - data encrypted at rest in Log Analytics, customer-managed keys optional, regional storage compliance.
* *Query* - RBAC controls query permissions, row-level security for sensitive data, audit logging of queries.
* *Export* - incident export to SIEM, continuous export to Storage/Event Hub, API access for custom integration.

Understanding these components and their interactions is essential for architecting effective Sentinel deployments, optimizing detection capabilities, and ensuring secure operations at scale.

== What are Data Collection Endpoints (DCEs) and Data Collection Rules (DCRs) in Microsoft Sentinel, and how do they work together?

DCEs and DCRs represent Microsoft's modern data ingestion architecture, providing centralized management, transformation, and routing of monitoring data.

*Data Collection Endpoints (DCEs)*:

*What they are*:

* *Regional endpoints* - Azure resources serving as ingestion points for monitoring data, entry point for agents and applications sending logs, regional for data residency and latency optimization.
* *Network access* - publicly accessible by default, private endpoint support for network isolation, TLS encryption for all connections.

*Purpose*:

* *Single point of ingestion* - agents/applications send to DCE instead of direct to workspace, enables centralized authentication and authorization, simplifies networking (single endpoint to allow).
* *Security boundary* - managed identity authentication, network isolation via Private Link, reduces exposure of Log Analytics workspaces.

*DCE structure*:

[,json]
----
{
  "name": "myDCE",
  "location": "eastus",
  "properties": {
    "networkAcls": {
      "publicNetworkAccess": "Enabled"
    },
    "configurationEndpoint": "https://mydce-abc123.eastus-1.handler.control.monitor.azure.com",
    "logsIngestionEndpoint": "https://mydce-abc123.eastus-1.ingest.monitor.azure.com"
  }
}
----

*Creating DCE*:

[,bash]
----
# Azure CLI
az monitor data-collection-endpoint create \
  --name "Production-DCE" \
  --resource-group "sentinel-rg" \
  --location "eastus" \
  --public-network-access "Enabled"

# With Private Link
az monitor data-collection-endpoint create \
  --name "Secure-DCE" \
  --resource-group "sentinel-rg" \
  --location "eastus" \
  --public-network-access "Disabled"

# Then create private endpoint connection
az network private-endpoint create \
  --name "dce-private-endpoint" \
  --resource-group "sentinel-rg" \
  --vnet-name "security-vnet" \
  --subnet "monitoring-subnet" \
  --private-connection-resource-id $DCE_ID \
  --group-id "configurationEndpoints" \
  --connection-name "dce-connection"
----

*Data Collection Rules (DCRs)*:

*What they are*:

* *Declarative rules* - JSON documents defining data collection configuration, specifies what data to collect, how to transform it, where to send it.
* *Centralized management* - single rule can apply to multiple resources, versioned and auditable, ARM template deployable.

*DCR components*:

*Data sources* - what data to collect:

[,json]
----
"dataSources": {
  "windowsEventLogs": [{
    "name": "SecurityEvents",
    "streams": ["Microsoft-SecurityEvent"],
    "xPathQueries": [
      "Security!*[System[(EventID=4625)]]",  // Failed logon
      "Security!*[System[(EventID=4624)]]"   // Successful logon
    ]
  }],
  "syslog": [{
    "name": "SyslogDataSource",
    "streams": ["Microsoft-Syslog"],
    "facilityNames": ["auth", "authpriv"],
    "logLevels": ["Warning", "Error", "Critical"]
  }],
  "performanceCounters": [{
    "name": "PerfCounters",
    "streams": ["Microsoft-Perf"],
    "samplingFrequencyInSeconds": 60,
    "counterSpecifiers": [
      "\\Processor(_Total)\\% Processor Time",
      "\\Memory\\Available Bytes"
    ]
  }]
}
----

*Transformations* - KQL to filter/modify data before ingestion:

[,json]
----
"transformKql": "source | where EventID in (4625, 4624, 4720, 4726) | extend AccountType = case(AccountType == '%%8272', 'User', AccountType == '%%8274', 'Computer', 'Unknown')"
----

*Destinations* - where to send data:

[,json]
----
"destinations": {
  "logAnalytics": [{
    "workspaceResourceId": "/subscriptions/{sub}/resourceGroups/{rg}/providers/Microsoft.OperationalInsights/workspaces/{workspace}",
    "name": "SentinelWorkspace"
  }]
},
"dataFlows": [{
  "streams": ["Microsoft-SecurityEvent"],
  "destinations": ["SentinelWorkspace"],
  "transformKql": "source | where EventID == 4625",
  "outputStream": "Microsoft-SecurityEvent"
}]
----

*Complete DCR example* (Security event collection):

[,json]
----
{
  "properties": {
    "dataSources": {
      "windowsEventLogs": [{
        "name": "SecurityEventsDataSource",
        "streams": ["Microsoft-SecurityEvent"],
        "xPathQueries": [
          "Security!*[System[(EventID=4625 or EventID=4624 or EventID=4720)]]"
        ]
      }]
    },
    "destinations": {
      "logAnalytics": [{
        "workspaceResourceId": "/subscriptions/abc-123/resourceGroups/sentinel-rg/providers/Microsoft.OperationalInsights/workspaces/sentinel-la",
        "name": "SentinelWorkspace"
      }]
    },
    "dataFlows": [{
      "streams": ["Microsoft-SecurityEvent"],
      "destinations": ["SentinelWorkspace"],
      "transformKql": "source | where EventID in (4625, 4624, 4720) | extend TimeCreated = TimeGenerated | project-away TimeGenerated | project-rename TimeGenerated = TimeCreated"
    }],
    "dataCollectionEndpointId": "/subscriptions/abc-123/resourceGroups/sentinel-rg/providers/Microsoft.Insights/dataCollectionEndpoints/Production-DCE"
  },
  "location": "eastus",
  "name": "SecurityEvents-DCR"
}
----

*How DCEs and DCRs work together*:

*Data flow*:

----
1. Agent/Application configured with:
   - DCE endpoint URL
   - DCR ID
   - Managed identity for auth

2. Agent connects to DCE:
   - Authenticates using managed identity
   - Downloads DCR configuration
   - Learns what data to collect

3. Agent collects data per DCR:
   - Collects specified events/logs
   - Applies local filtering if configured

4. Agent sends data to DCE:
   - Batches data for efficiency
   - Sends to DCE ingestion endpoint
   - TLS encrypted

5. DCE processes data:
   - Applies DCR transformations (KQL)
   - Routes to destinations (Log Analytics)
   - Retries on failure

6. Data lands in Sentinel:
   - Appears in specified table
   - Available for analytics rules
   - Queryable immediately
----

*Security benefits of DCE/DCR architecture*:

*Network isolation*:

[,hcl]
----
# Terraform example: Private DCE
resource "azurerm_monitor_data_collection_endpoint" "secure" {
  name                          = "secure-dce"
  resource_group_name           = azurerm_resource_group.sentinel.name
  location                      = "eastus"
  public_network_access_enabled = false  # Force private endpoint
}

resource "azurerm_private_endpoint" "dce" {
  name                = "dce-private-endpoint"
  resource_group_name = azurerm_resource_group.sentinel.name
  location            = "eastus"
  subnet_id           = azurerm_subnet.monitoring.id

  private_service_connection {
    name                           = "dce-connection"
    private_connection_resource_id = azurerm_monitor_data_collection_endpoint.secure.id
    subresource_names              = ["configurationAccess"]
  }
}
----

*Data transformation for cost savings*:

[,json]
----
// Filter out noisy events before ingestion (cost savings)
{
  "transformKql": "source | where EventID != 5156 and EventID != 5158 | where AccountName !contains 'NT AUTHORITY'"
}

// Result: 40-60% reduction in ingestion volume
// Savings: ~$1000/month per 100GB/day filtered
----

*Sensitive data filtering*:

[,json]
----
// Remove PII before ingestion (compliance)
{
  "transformKql": "source | extend Message = replace_regex(Message, @'\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\\b', '***EMAIL***') | extend Message = replace_regex(Message, @'\\b\\d{3}-\\d{2}-\\d{4}\\b', '***SSN***')"
}
----

*Role-based access*:

[,bash]
----
# DCR uses RBAC for access control
az role assignment create \
  --assignee $VM_MANAGED_IDENTITY \
  --role "Monitoring Metrics Publisher" \
  --scope $DCR_ID

# Least privilege: VMs can only send to assigned DCRs
----

*Use cases*:

*Multi-workspace routing*:

[,json]
----
// Send security events to Security workspace
// Send performance to Operations workspace
{
  "destinations": {
    "logAnalytics": [
      {
        "workspaceResourceId": "/subscriptions/.../workspaces/security-ws",
        "name": "SecurityDest"
      },
      {
        "workspaceResourceId": "/subscriptions/.../workspaces/ops-ws",
        "name": "OpsDest"
      }
    ]
  },
  "dataFlows": [
    {
      "streams": ["Microsoft-SecurityEvent"],
      "destinations": ["SecurityDest"]
    },
    {
      "streams": ["Microsoft-Perf"],
      "destinations": ["OpsDest"]
    }
  ]
}
----

*Regional compliance*:

[,bash]
----
// EU data to EU workspace, US data to US workspace
EU-DCE (westeurope) â†’ DCR â†’ EU-Workspace (westeurope)
US-DCE (eastus) â†’ DCR â†’ US-Workspace (eastus)

// Data never leaves region
----

*Cost optimization*:

[,bash]
----
// Transformation reducing 100GB/day to 40GB/day
Before: 100GB/day Ã— $2.30/GB = $230/day = $6,900/month
After:  40GB/day Ã— $2.30/GB = $92/day = $2,760/month
Savings: $4,140/month (60% reduction)
----

*Best practices*:

* Use DCEs for all new deployments (legacy direct ingestion being deprecated)
* One DCE per region for latency optimization
* Private endpoints for production workloads
* Transformation to filter noisy/unnecessary data
* Separate DCRs per environment (dev/staging/prod)
* Version control DCR definitions (ARM templates)
* Test transformations in dev before production
* Monitor DCR performance and costs
* Regular review of transformation efficiency

DCEs and DCRs represent modern, secure, cost-effective data ingestion - essential for large-scale Sentinel deployments requiring granular control, compliance, and cost optimization.

== How do you use Azure Logic Apps as playbooks in Microsoft Sentinel for security orchestration and automated response?

Azure Logic Apps serve as Sentinel's automation engine, enabling Security Orchestration, Automation, and Response (SOAR) capabilities through visual workflow design.

*Logic Apps in Sentinel context*:

*What they are*:

* *Workflow automation* - visual designer for creating automated processes, 400+ connectors to services and applications, code-free or code-enabled (inline code, Azure Functions).
* *Playbooks* - Logic Apps designed for security automation, triggered by Sentinel incidents or alerts, perform investigation, enrichment, and response actions.

*Common playbook scenarios*:

*Scenario 1: Automated enrichment*:

[,bash]
----
Incident created (Suspicious IP detected)
â†’ Get IP reputation from VirusTotal
â†’ Get geolocation from IP2Location
â†’ Query threat intelligence feeds
â†’ Add all findings as comment to incident
â†’ If IP is malicious:
  â†’ Escalate incident severity to High
  â†’ Assign to senior analyst
  â†’ Send Teams notification
----

*Scenario 2: Automated response*:

[,bash]
----
Alert: Compromised user account
â†’ Get user details from Azure AD
â†’ Check recent sign-ins
â†’ If suspicious activity confirmed:
  â†’ Revoke all user sessions
  â†’ Reset password (force change)
  â†’ Disable account
  â†’ Notify user's manager
  â†’ Create ServiceNow incident
  â†’ Add timeline to Sentinel incident
----

*Scenario 3: Threat hunting automation*:

[,bash]
----
Scheduled trigger (daily)
â†’ Query Sentinel for IOCs from last 24h
â†’ For each IOC:
  â†’ Check if seen in other data sources
  â†’ Query VirusTotal for reputation
  â†’ If malicious and widespread:
    â†’ Create high-priority incident
    â†’ Block IOC in firewall
    â†’ Alert SOC team
----

*Creating a Sentinel playbook*:

*Step 1: Create Logic App with Sentinel trigger*:

[,json]
----
// ARM template excerpt
{
  "type": "Microsoft.Logic/workflows",
  "apiVersion": "2019-05-01",
  "name": "Block-MaliciousIP",
  "location": "[resourceGroup().location]",
  "identity": {
    "type": "SystemAssigned"  // Managed identity for Sentinel access
  },
  "properties": {
    "definition": {
      "$schema": "https://schema.management.azure.com/providers/Microsoft.Logic/schemas/2016-06-01/workflowdefinition.json#",
      "triggers": {
        "Microsoft_Sentinel_incident": {
          "type": "ApiConnectionWebhook",
          "inputs": {
            "host": {
              "connection": {
                "name": "@parameters('$connections')['azuresentinel']['connectionId']"
              }
            },
            "body": {
              "callback_url": "@{listCallbackUrl()}"
            },
            "path": "/incident-creation"
          }
        }
      },
      "actions": {
        // Playbook actions defined here
      }
    }
  }
}
----

*Step 2: Grant permissions*:

[,bash]
----
# Playbook needs Sentinel Responder role
az role assignment create \
  --assignee $LOGIC_APP_IDENTITY \
  --role "Microsoft Sentinel Responder" \
  --scope "/subscriptions/{sub}/resourceGroups/{rg}/providers/Microsoft.OperationalInsights/workspaces/{workspace}"

# If interacting with Azure resources, grant additional permissions
az role assignment create \
  --assignee $LOGIC_APP_IDENTITY \
  --role "Network Contributor" \  # For firewall rules
  --scope "/subscriptions/{sub}/resourceGroups/{rg}"
----

*Step 3: Design playbook logic*:

*Example: IP enrichment and blocking playbook*:

*Trigger* - Microsoft Sentinel incident (when incident created):

[,json]
----
{
  "triggerCondition": "properties/Severity eq 'High'",  // Only high severity
  "filter": "properties/Title contains 'Malicious IP'"  // Specific incidents
}
----

*Action 1* - Get IP entities from incident:

[,json]
----
{
  "type": "ParseJson",
  "inputs": {
    "content": "@triggerBody()?['object']?['properties']?['relatedEntities']",
    "schema": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "kind": {"type": "string"},
          "properties": {
            "type": "object",
            "properties": {
              "address": {"type": "string"}
            }
          }
        }
      }
    }
  }
}
----

*Action 2* - For each IP, check VirusTotal:

[,json]
----
{
  "type": "Foreach",
  "foreach": "@body('Parse_Entities')?['entities']",
  "actions": {
    "HTTP_VirusTotal": {
      "type": "Http",
      "inputs": {
        "method": "GET",
        "uri": "https://www.virustotal.com/api/v3/ip_addresses/@{items('For_each_IP')?['properties']?['address']}",
        "headers": {
          "x-apikey": "@parameters('VirusTotal_API_Key')"
        }
      }
    }
  }
}
----

*Action 3* - Parse VirusTotal response:

[,json]
----
{
  "type": "ParseJson",
  "inputs": {
    "content": "@body('HTTP_VirusTotal')",
    "schema": {
      "type": "object",
      "properties": {
        "data": {
          "type": "object",
          "properties": {
            "attributes": {
              "type": "object",
              "properties": {
                "last_analysis_stats": {
                  "type": "object",
                  "properties": {
                    "malicious": {"type": "integer"}
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
----

*Action 4* - Conditional: If malicious:

[,json]
----
{
  "type": "If",
  "expression": {
    "and": [{
      "greater": [
        "@body('Parse_VT')?['data']?['attributes']?['last_analysis_stats']?['malicious']",
        5  // More than 5 vendors flagged as malicious
      ]
    }]
  },
  "actions": {
    "Block_IP_in_Firewall": {
      // Action to block IP
    },
    "Add_Comment_to_Incident": {
      // Document action taken
    },
    "Send_Teams_Notification": {
      // Alert SOC
    }
  },
  "runAfter": {
    "Parse_VT": ["Succeeded"]
  }
}
----

*Action 5* - Block IP in Azure Firewall:

[,json]
----
{
  "type": "ApiConnection",
  "inputs": {
    "host": {
      "connection": {
        "name": "@parameters('$connections')['azurefirewall']['connectionId']"
      }
    },
    "method": "put",
    "path": "/subscriptions/{sub}/resourceGroups/{rg}/providers/Microsoft.Network/azureFirewalls/{firewall}/ipConfigurations/{config}",
    "body": {
      "properties": {
        "networkRuleCollections": [{
          "name": "Block-Malicious-IPs",
          "properties": {
            "priority": 100,
            "action": {"type": "Deny"},
            "rules": [{
              "name": "Block-@{items('For_each_IP')?['properties']?['address']}",
              "sourceAddresses": ["@{items('For_each_IP')?['properties']?['address']}"],
              "destinationAddresses": ["*"],
              "destinationPorts": ["*"],
              "protocols": ["Any"]
            }]
          }
        }]
      }
    }
  }
}
----

*Action 6* - Add comment to Sentinel incident:

[,json]
----
{
  "type": "ApiConnection",
  "inputs": {
    "host": {
      "connection": {
        "name": "@parameters('$connections')['azuresentinel']['connectionId']"
      }
    },
    "method": "post",
    "path": "/Incidents/Comment",
    "body": {
      "incidentId": "@triggerBody()?['object']?['id']",
      "message": "IP @{items('For_each_IP')?['properties']?['address']} identified as malicious by VirusTotal (@{body('Parse_VT')?['data']?['attributes']?['last_analysis_stats']?['malicious']} vendors). Blocked in Azure Firewall. Playbook: Block-MaliciousIP"
    }
  }
}
----

*Action 7* - Send Teams notification:

[,json]
----
{
  "type": "ApiConnection",
  "inputs": {
    "host": {
      "connection": {
        "name": "@parameters('$connections')['teams']['connectionId']"
      }
    },
    "method": "post",
    "path": "/flowbot/actions/postCardToChannel",
    "body": {
      "recipient": {
        "channelId": "security-alerts-channel-id"
      },
      "card": {
        "type": "AdaptiveCard",
        "body": [{
          "type": "TextBlock",
          "size": "Large",
          "weight": "Bolder",
          "text": "ðŸš¨ Malicious IP Blocked"
        }, {
          "type": "FactSet",
          "facts": [
            {"title": "IP Address", "value": "@{items('For_each_IP')?['properties']?['address']}"},
            {"title": "Incident", "value": "@{triggerBody()?['object']?['properties']?['title']}"},
            {"title": "VirusTotal Score", "value": "@{body('Parse_VT')?['data']?['attributes']?['last_analysis_stats']?['malicious']}/70"}
          ]
        }],
        "actions": [{
          "type": "Action.OpenUrl",
          "title": "View Incident",
          "url": "@{triggerBody()?['object']?['properties']?['incidentUrl']}"
        }]
      }
    }
  }
}
----

*Advanced playbook patterns*:

*Pattern 1: User account compromise response*:

[,bash]
----
Trigger: Alert "Impossible travel"
â†’ Get user from alert entities
â†’ Query Azure AD for user's recent sign-ins (last 24h)
â†’ Check for:
  - Multiple countries
  - Unusual user agents
  - Failed MFA attempts
â†’ If confirmed compromise:
  - Revoke all refresh tokens (force re-auth)
  - Reset password with force change
  - Disable account
  - Get user's manager from Azure AD
  - Email manager about compromise
  - Create PagerDuty incident
  - Add detailed timeline to Sentinel incident
â†’ Else (false positive):
  - Add comment explaining why benign
  - Lower severity to Low
----

*Pattern 2: Automated threat hunting*:

[,bash]
----
Scheduled trigger (every 6 hours)
â†’ Query Sentinel for new IOCs from threat intel
â†’ For each IOC:
  - Search across all log sources (last 30 days)
  - If found:
    * Create high-severity incident
    * Enrich with context (which systems, when, frequency)
    * Check if still active
    * If active: Block IOC in security controls
    * Assign to threat hunter
    * Add to watchlist for ongoing monitoring
----

*Pattern 3: Compliance automation*:

[,bash]
----
Trigger: Azure Activity log (role assignment change)
â†’ Get assignment details
â†’ Check if privileged role (Global Admin, etc.)
â†’ If privileged:
  - Verify against approved change ticket
  - If no ticket:
    * Revert assignment
    * Create incident
    * Alert compliance team
  - If ticket exists:
    * Verify assignment matches ticket
    * Add to compliance audit log
    * Schedule review in 90 days
----

*Security best practices for playbooks*:

*Authentication and authorization*:

[,bash]
----
# Use managed identity (not service principal with secrets)
# Logic App automatically gets managed identity

# Grant minimum permissions
az role assignment create \
  --assignee $LOGIC_APP_IDENTITY \
  --role "Microsoft Sentinel Responder" \  # Not Contributor
  --scope $SENTINEL_WORKSPACE_ID

# For API connections, use Key Vault for secrets
# Never hardcode API keys in playbook
----

*Error handling*:

[,json]
----
// Add error handling to all actions
{
  "type": "Scope",
  "actions": {
    "Try_Block_IP": {
      // Risky action
    }
  },
  "runAfter": {},
  "else": {
    "actions": {
      "Catch_Add_Comment": {
        "type": "ApiConnection",
        "inputs": {
          "body": {
            "message": "Failed to block IP: @{result('Try_Block_IP')[0]['error']['message']}"
          }
        }
      },
      "Alert_Administrator": {
        // Send alert about failure
      }
    }
  }
}
----

*Testing and validation*:

[,bash]
----
# Test playbooks in dev environment first
# Use test incidents with known data

# Manual trigger for testing
az logic app trigger run \
  --resource-group sentinel-rg \
  --name Block-MaliciousIP \
  --trigger-name manual

# Monitor execution
az logic app show \
  --resource-group sentinel-rg \
  --name Block-MaliciousIP \
  --query "state"
----

*Playbook governance*:

* Version control playbook definitions (ARM templates in Git)
* Code review for playbook changes
* Separate dev/test/prod playbooks
* Audit playbook executions
* Monitor playbook failures and performance
* Documentation for each playbook (what it does, when it runs, expected behavior)
* Regular review and optimization
* Disable unused playbooks

*Cost optimization*:

* Logic Apps charged per action execution
* Minimize unnecessary actions
* Use conditions to skip unnecessary steps
* Batch operations where possible
* Consider Standard tier for high-volume playbooks (cheaper per action)

Logic Apps playbooks transform Sentinel from detection platform to comprehensive SOAR solution - enabling automated response at machine speed while maintaining human oversight for critical decisions.

== How do you use Azure Functions with Microsoft Sentinel for custom security automation and advanced processing?

Azure Functions provide serverless compute for complex security automation scenarios that exceed Logic Apps' capabilities, enabling custom code execution triggered by Sentinel events.

*When to use Functions vs. Logic Apps*:

*Use Azure Functions when*:

* *Complex logic* - advanced algorithms, custom parsing, mathematical computations.
* *Performance critical* - need faster execution than Logic Apps, processing large data volumes, real-time requirements.
* *Custom integrations* - API not available in Logic Apps connectors, proprietary protocols, legacy systems.
* *Cost optimization* - high-volume scenarios where Functions cheaper (millions of executions).
* *Code reuse* - existing libraries or code to leverage, team prefers code over visual design.

*Use Logic Apps when*:

* *Visual workflow* - non-developers need to understand/modify, clear business process flow.
* *Many integrations* - leveraging existing connectors, standard SaaS integrations.
* *Approval workflows* - human-in-the-loop scenarios, email approvals.
* *Simple automation* - straightforward if/then logic, enrichment and notification.

*Azure Functions architecture for Sentinel*:

*Trigger types*:

* *HTTP trigger* - called from Logic Apps or Sentinel playbooks via webhook, API endpoint for external systems.
* *Timer trigger* - scheduled execution for recurring tasks (threat hunting, cleanup).
* *Queue trigger* - process incidents from Azure Queue, enables async processing and load leveling.
* *Event Grid trigger* - react to Azure events, workspace changes.

*Common Function scenarios*:

*Scenario 1: Advanced log parsing and enrichment*:

[,python]
----
import azure.functions as func
import json
import re
import hashlib
from typing import Dict, List

def main(req: func.HttpRequest) -> func.HttpResponse:
    """
    Parse complex firewall logs and enrich with threat intelligence
    Called from Sentinel Analytics Rule or Logic App
    """
    try:
        # Get incident data
        incident = req.get_json()

        # Extract firewall logs from incident
        logs = extract_firewall_logs(incident)

        # Parse custom format
        parsed_events = []
        for log in logs:
            event = parse_custom_firewall_format(log)

            # Calculate hash for deduplication
            event['hash'] = hashlib.sha256(
                f"{event['src_ip']}{event['dst_ip']}{event['port']}".encode()
            ).hexdigest()

            # Enrich with GeoIP
            event['src_country'] = get_geoip(event['src_ip'])
            event['dst_country'] = get_geoip(event['dst_ip'])

            # Check threat feeds
            event['threat_score'] = check_threat_feeds(event['src_ip'])

            # Classify traffic
            event['classification'] = classify_traffic(event)

            parsed_events.append(event)

        # Aggregate and analyze
        analysis = analyze_traffic_patterns(parsed_events)

        # Return enriched data
        return func.HttpResponse(
            json.dumps({
                'parsed_events': parsed_events,
                'analysis': analysis,
                'high_risk_ips': [e for e in parsed_events if e['threat_score'] > 80]
            }),
            mimetype="application/json",
            status_code=200
        )

    except Exception as e:
        return func.HttpResponse(
            json.dumps({'error': str(e)}),
            status_code=500
        )

def parse_custom_firewall_format(log: str) -> Dict:
    """Parse proprietary firewall log format"""
    pattern = r'(\d+\.\d+\.\d+\.\d+):(\d+) -> (\d+\.\d+\.\d+\.\d+):(\d+) \[(\w+)\]'
    match = re.match(pattern, log)

    if match:
        return {
            'src_ip': match.group(1),
            'src_port': int(match.group(2)),
            'dst_ip': match.group(3),
            'dst_port': int(match.group(4)),
            'action': match.group(5)
        }
    return {}

def check_threat_feeds(ip: str) -> int:
    """Check multiple threat intelligence sources"""
    score = 0

    # Check VirusTotal
    vt_score = query_virustotal(ip)
    score += vt_score * 10

    # Check AbuseIPDB
    abuse_score = query_abuseipdb(ip)
    score += abuse_score

    # Check internal threat feed
    internal_score = query_internal_feed(ip)
    score += internal_score

    return min(score, 100)  # Cap at 100

def analyze_traffic_patterns(events: List[Dict]) -> Dict:
    """Detect suspicious patterns"""
    analysis = {
        'port_scanning': detect_port_scanning(events),
        'data_exfiltration': detect_exfiltration(events),
        'beaconing': detect_beaconing(events),
        'lateral_movement': detect_lateral_movement(events)
    }
    return analysis
----

*Scenario 2: Machine learning-based anomaly detection*:

[,python]
----
import azure.functions as func
import numpy as np
from sklearn.ensemble import IsolationForest
from azure.identity import DefaultAzureCredential
from azure.keyvault.secrets import SecretClient
from azure.storage.blob import BlobServiceClient
import pickle
import json

# Global model (loaded once)
MODEL = None

def main(req: func.HttpRequest) -> func.HttpResponse:
    """
    Apply ML model to detect anomalous user behavior
    """
    global MODEL

    # Load model if not cached
    if MODEL is None:
        MODEL = load_model_from_blob()

    # Get user activity data from request
    user_activities = req.get_json()

    # Extract features
    features = extract_behavior_features(user_activities)

    # Predict anomalies
    predictions = MODEL.predict(features)
    anomaly_scores = MODEL.score_samples(features)

    # Identify anomalies
    anomalies = []
    for i, (pred, score) in enumerate(zip(predictions, anomaly_scores)):
        if pred == -1:  # Anomaly
            anomalies.append({
                'user': user_activities[i]['user'],
                'timestamp': user_activities[i]['timestamp'],
                'anomaly_score': float(score),
                'activities': user_activities[i]['activities'],
                'reason': explain_anomaly(features[i], MODEL)
            })

    return func.HttpResponse(
        json.dumps({
            'anomalies_detected': len(anomalies),
            'anomalies': anomalies,
            'model_version': get_model_version()
        }),
        mimetype="application/json"
    )

def extract_behavior_features(activities: List[Dict]) -> np.ndarray:
    """Extract behavioral features for ML model"""
    features = []

    for activity in activities:
        feature_vector = [
            activity['login_count'],
            activity['failed_login_count'],
            activity['unique_ips'],
            activity['unique_locations'],
            activity['privileged_actions'],
            activity['data_accessed_gb'],
            activity['after_hours_activity'],
            activity['weekend_activity'],
            len(activity['countries_accessed']),
            activity['mfa_failures']
        ]
        features.append(feature_vector)

    return np.array(features)

def load_model_from_blob() -> IsolationForest:
    """Load trained model from Azure Blob Storage"""
    # Get storage connection from Key Vault
    credential = DefaultAzureCredential()
    kv_client = SecretClient(
        vault_url="https://sentinel-kv.vault.azure.net",
        credential=credential
    )

    connection_string = kv_client.get_secret("storage-connection-string").value

    # Download model
    blob_client = BlobServiceClient.from_connection_string(connection_string)
    container_client = blob_client.get_container_client("ml-models")
    blob = container_client.get_blob_client("user-behavior-model.pkl")

    model_data = blob.download_blob().readall()
    model = pickle.loads(model_data)

    return model

def explain_anomaly(features: np.ndarray, model: IsolationForest) -> str:
    """Generate human-readable explanation of anomaly"""
    feature_names = [
        'login_count', 'failed_logins', 'unique_ips', 'unique_locations',
        'privileged_actions', 'data_accessed', 'after_hours', 'weekend',
        'countries', 'mfa_failures'
    ]

    # Find most anomalous features
    feature_importance = np.abs(features - model.tree_.feature_threshold)
    top_features = np.argsort(feature_importance)[-3:]

    explanations = []
    for idx in top_features:
        explanations.append(f"Unusual {feature_names[idx]}: {features[idx]}")

    return "; ".join(explanations)
----

*Scenario 3: Custom STIX/TAXII threat intelligence ingestion*:

[,python]
----
import azure.functions as func
from stix2 import FileSystemSource, Filter
from taxii2client.v20 import Server, Collection
from azure.monitor.ingestion import LogsIngestionClient
from azure.identity import DefaultAzureCredential
import json
from datetime import datetime, timedelta

def main(timer: func.TimerRequest) -> None:
    """
    Scheduled function to ingest threat intel from STIX/TAXII feeds
    Runs every hour
    """
    if timer.past_due:
        logging.info('Timer is past due!')

    # Connect to TAXII server
    server = Server(
        "https://threatintel.example.com/taxii/",
        user="api_user",
        password=get_secret("taxii-password")
    )

    # Get collection
    api_root = server.api_roots[0]
    collection = Collection(
        f"{api_root.url}collections/threat-indicators/",
        user="api_user",
        password=get_secret("taxii-password")
    )

    # Fetch indicators from last hour
    added_after = datetime.utcnow() - timedelta(hours=1)
    indicators = collection.get_objects(added_after=added_after)

    # Parse and transform indicators
    sentinel_indicators = []
    for indicator in indicators.get('objects', []):
        if indicator['type'] == 'indicator':
            sentinel_ind = transform_stix_to_sentinel(indicator)
            sentinel_indicators.append(sentinel_ind)

    # Ingest to Sentinel
    if sentinel_indicators:
        ingest_to_sentinel(sentinel_indicators)
        logging.info(f"Ingested {len(sentinel_indicators)} threat indicators")

def transform_stix_to_sentinel(stix_indicator: dict) -> dict:
    """Transform STIX indicator to Sentinel ThreatIntelligenceIndicator format"""
    return {
        'TimeGenerated': datetime.utcnow().isoformat(),
        'IndicatorId': stix_indicator['id'],
        'ThreatType': stix_indicator.get('labels', ['unknown'])[0],
        'Description': stix_indicator.get('description', ''),
        'Pattern': stix_indicator.get('pattern', ''),
        'PatternType': stix_indicator.get('pattern_type', ''),
        'ValidFrom': stix_indicator.get('valid_from', ''),
        'ValidUntil': stix_indicator.get('valid_until', ''),
        'Confidence': stix_indicator.get('confidence', 50),
        'ThreatSeverity': calculate_severity(stix_indicator),
        'ExternalReferences': json.dumps(stix_indicator.get('external_references', [])),
        'Tags': json.dumps(stix_indicator.get('labels', []))
    }

def ingest_to_sentinel(indicators: list):
    """Ingest indicators to Sentinel using Logs Ingestion API"""
    credential = DefaultAzureCredential()
    client = LogsIngestionClient(
        endpoint="https://data-collection-endpoint.eastus-1.ingest.monitor.azure.com",
        credential=credential
    )

    # Ingest in batches of 1000
    batch_size = 1000
    for i in range(0, len(indicators), batch_size):
        batch = indicators[i:i+batch_size]

        client.upload(
            rule_id="/subscriptions/.../dataCollectionRules/ThreatIntel-DCR",
            stream_name="Custom-ThreatIntelligenceIndicator_CL",
            logs=batch
        )
----

*Scenario 4: Advanced incident correlation*:

[,python]
----
import azure.functions as func
from azure.kusto.data import KustoClient, KustoConnectionStringBuilder
from azure.kusto.data.exceptions import KustoServiceError
import networkx as nx
from datetime import datetime, timedelta
import json

def main(req: func.HttpRequest) -> func.HttpResponse:
    """
    Correlate multiple incidents to identify coordinated attacks
    """
    # Get recent incidents
    incidents = req.get_json().get('incidents', [])

    # Build correlation graph
    graph = build_correlation_graph(incidents)

    # Find connected components (attack campaigns)
    campaigns = find_attack_campaigns(graph)

    # Analyze each campaign
    campaign_analysis = []
    for campaign in campaigns:
        analysis = analyze_campaign(campaign, incidents)
        campaign_analysis.append(analysis)

    # Generate recommendations
    recommendations = generate_recommendations(campaign_analysis)

    return func.HttpResponse(
        json.dumps({
            'campaigns_detected': len(campaigns),
            'campaigns': campaign_analysis,
            'recommendations': recommendations
        }),
        mimetype="application/json"
    )

def build_correlation_graph(incidents: list) -> nx.Graph:
    """Build graph of correlated incidents"""
    G = nx.Graph()

    # Add incidents as nodes
    for incident in incidents:
        G.add_node(incident['id'], **incident)

    # Add edges for correlations
    for i, inc1 in enumerate(incidents):
        for inc2 in incidents[i+1:]:
            # Check correlation criteria
            correlation_score = calculate_correlation(inc1, inc2)

            if correlation_score > 0.7:  # Threshold
                G.add_edge(inc1['id'], inc2['id'], weight=correlation_score)

    return G

def calculate_correlation(inc1: dict, inc2: dict) -> float:
    """Calculate correlation score between incidents"""
    score = 0.0

    # Shared entities
    entities1 = set(e['id'] for e in inc1.get('entities', []))
    entities2 = set(e['id'] for e in inc2.get('entities', []))
    shared_entities = entities1.intersection(entities2)

    if shared_entities:
        score += 0.4 * (len(shared_entities) / max(len(entities1), len(entities2)))

    # Time proximity
    time1 = datetime.fromisoformat(inc1['created_time'])
    time2 = datetime.fromisoformat(inc2['created_time'])
    time_diff = abs((time1 - time2).total_seconds())

    if time_diff < 3600:  # Within 1 hour
        score += 0.3
    elif time_diff < 86400:  # Within 1 day
        score += 0.15

    # Similar tactics (MITRE ATT&CK)
    tactics1 = set(inc1.get('tactics', []))
    tactics2 = set(inc2.get('tactics', []))
    shared_tactics = tactics1.intersection(tactics2)

    if shared_tactics:
        score += 0.3 * (len(shared_tactics) / max(len(tactics1), len(tactics2)))

    return score

def find_attack_campaigns(graph: nx.Graph) -> list:
    """Identify connected components as attack campaigns"""
    campaigns = []

    for component in nx.connected_components(graph):
        if len(component) >= 2:  # At least 2 correlated incidents
            subgraph = graph.subgraph(component)
            campaigns.append({
                'incidents': list(component),
                'incident_count': len(component),
                'avg_correlation': nx.average_node_connectivity(subgraph)
            })

    return campaigns

def analyze_campaign(campaign: dict, incidents: list) -> dict:
    """Analyze attack campaign characteristics"""
    campaign_incidents = [i for i in incidents if i['id'] in campaign['incidents']]

    # Extract all entities
    all_entities = {}
    for inc in campaign_incidents:
        for entity in inc.get('entities', []):
            entity_id = entity['id']
            if entity_id not in all_entities:
                all_entities[entity_id] = entity

    # Timeline
    timeline = sorted([
        {
            'time': inc['created_time'],
            'incident': inc['title'],
            'severity': inc['severity']
        }
        for inc in campaign_incidents
    ], key=lambda x: x['time'])

    # Tactics progression
    all_tactics = []
    for inc in campaign_incidents:
        all_tactics.extend(inc.get('tactics', []))

    tactics_progression = analyze_tactics_progression(all_tactics)

    return {
        'campaign_id': f"CAMP-{hash(''.join(campaign['incidents'])) % 10000}",
        'incident_count': campaign['incident_count'],
        'entities': all_entities,
        'timeline': timeline,
        'tactics_progression': tactics_progression,
        'attack_stage': determine_attack_stage(tactics_progression),
        'severity': max(i['severity'] for i in campaign_incidents),
        'recommended_actions': generate_campaign_response(campaign_incidents)
    }
----

*Function deployment and configuration*:

*Function App setup*:

[,bash]
----
# Create Function App with managed identity
az functionapp create \
  --name sentinel-functions \
  --resource-group sentinel-rg \
  --storage-account sentinelstorage \
  --consumption-plan-location eastus \
  --runtime python \
  --runtime-version 3.9 \
  --functions-version 4 \
  --assign-identity [system]

# Grant Sentinel permissions
FUNCTION_IDENTITY=$(az functionapp identity show \
  --name sentinel-functions \
  --resource-group sentinel-rg \
  --query principalId -o tsv)

az role assignment create \
  --assignee $FUNCTION_IDENTITY \
  --role "Microsoft Sentinel Contributor" \
  --scope "/subscriptions/{sub}/resourceGroups/sentinel-rg/providers/Microsoft.OperationalInsights/workspaces/sentinel-ws"
----

*Configuration (local.settings.json)*:

[,json]
----
{
  "IsEncrypted": false,
  "Values": {
    "AzureWebJobsStorage": "UseDevelopmentStorage=true",
    "FUNCTIONS_WORKER_RUNTIME": "python",
    "SENTINEL_WORKSPACE_ID": "abc-123-def",
    "KEY_VAULT_URL": "https://sentinel-kv.vault.azure.net",
    "LOG_ANALYTICS_WORKSPACE_ID": "workspace-guid",
    "DATA_COLLECTION_ENDPOINT": "https://dce.eastus-1.ingest.monitor.azure.com"
  }
}
----

*Integration with Logic Apps*:

[,json]
----
// Logic App calling Azure Function
{
  "type": "Function",
  "inputs": {
    "function": {
      "id": "/subscriptions/{sub}/resourceGroups/sentinel-rg/providers/Microsoft.Web/sites/sentinel-functions/functions/ParseComplexLogs"
    },
    "method": "POST",
    "body": {
      "incident_id": "@triggerBody()?['object']?['id']",
      "logs": "@body('Get_Firewall_Logs')"
    }
  },
  "runAfter": {
    "Get_Firewall_Logs": ["Succeeded"]
  }
}
----

*Security best practices for Functions*:

*Secrets management*:

[,python]
----
from azure.identity import DefaultAzureCredential
from azure.keyvault.secrets import SecretClient

def get_secret(secret_name: str) -> str:
    """Retrieve secret from Key Vault using managed identity"""
    credential = DefaultAzureCredential()
    kv_url = os.environ['KEY_VAULT_URL']
    client = SecretClient(vault_url=kv_url, credential=credential)
    return client.get_secret(secret_name).value

# Never hardcode secrets
api_key = get_secret("virustotal-api-key")
----

*Authentication*:

[,python]
----
# Require authentication for HTTP triggers
{
  "bindings": [{
    "authLevel": "function",  // Require function key
    "type": "httpTrigger",
    "direction": "in",
    "name": "req",
    "methods": ["post"]
  }]
}

# Or use Azure AD authentication
{
  "authLevel": "anonymous",  // But configure App Service Auth
  "type": "httpTrigger"
}
----

*Error handling and logging*:

[,python]
----
import logging
import azure.functions as func

def main(req: func.HttpRequest) -> func.HttpResponse:
    try:
        # Processing logic
        result = process_data(req.get_json())

        logging.info(f"Successfully processed {len(result)} items")

        return func.HttpResponse(
            json.dumps(result),
            status_code=200
        )

    except ValueError as e:
        logging.error(f"Invalid input: {str(e)}")
        return func.HttpResponse(
            json.dumps({'error': 'Invalid input'}),
            status_code=400
        )

    except Exception as e:
        logging.exception("Unexpected error occurred")
        return func.HttpResponse(
            json.dumps({'error': 'Internal server error'}),
            status_code=500
        )
----

*Best practices*:

* Use managed identity for authentication (no secrets)
* Store secrets in Key Vault
* Implement comprehensive error handling
* Log all operations for audit trail
* Use Application Insights for monitoring
* Set appropriate timeout values
* Implement retry logic for external APIs
* Version control function code
* Separate dev/test/prod environments
* Monitor function performance and costs

Azure Functions enable sophisticated security automation beyond Logic Apps' capabilities - essential for complex parsing, machine learning, and custom integrations in advanced security operations.

== How do you secure and manage secrets in Microsoft Sentinel using Azure Key Vault?

Azure Key Vault is essential for secure secret management in Sentinel deployments, protecting credentials, API keys, certificates, and other sensitive data used in automation and integrations.

*Why Key Vault for Sentinel*:

*Security requirements*:

* *No hardcoded secrets* - API keys, passwords, connection strings never in code, playbooks, functions, or configuration files.
* *Centralized management* - single source of truth for all secrets, rotation and expiration policies, audit trail of secret access.
* *Access control* - granular RBAC on secret access, managed identities for authentication, conditional access policies.

*Common secrets in Sentinel*:

* API keys (VirusTotal, AbuseIPDB, threat feeds)
* Service principal credentials
* Database connection strings
* Third-party service tokens
* Certificate private keys
* Webhook URLs with tokens
* SMTP passwords for email notifications

*Key Vault architecture for Sentinel*:

*Setup structure*:

[,bash]
----
Key Vault: sentinel-production-kv
â”œâ”€â”€ Secrets
â”‚   â”œâ”€â”€ virustotal-api-key
â”‚   â”œâ”€â”€ abuseipdb-api-key
â”‚   â”œâ”€â”€ pagerduty-token
â”‚   â”œâ”€â”€ slack-webhook-url
â”‚   â”œâ”€â”€ servicedesk-api-key
â”‚   â”œâ”€â”€ smtp-password
â”‚   â””â”€â”€ database-connection-string
â”œâ”€â”€ Keys
â”‚   â”œâ”€â”€ data-encryption-key
â”‚   â””â”€â”€ signing-key
â””â”€â”€ Certificates
    â”œâ”€â”€ api-client-cert
    â””â”€â”€ webhook-tls-cert
----

*Creating and configuring Key Vault*:

[,bash]
----
# Create Key Vault
az keyvault create \
  --name sentinel-production-kv \
  --resource-group sentinel-rg \
  --location eastus \
  --enable-soft-delete true \
  --enable-purge-protection true \  # Prevent permanent deletion
  --enable-rbac-authorization true  # Use Azure RBAC (not access policies)

# Enable diagnostic logging
az monitor diagnostic-settings create \
  --name kv-diagnostics \
  --resource "/subscriptions/{sub}/resourceGroups/sentinel-rg/providers/Microsoft.KeyVault/vaults/sentinel-production-kv" \
  --workspace "/subscriptions/{sub}/resourceGroups/sentinel-rg/providers/Microsoft.OperationalInsights/workspaces/sentinel-ws" \
  --logs '[
    {
      "category": "AuditEvent",
      "enabled": true
    }
  ]'

# Store secrets
az keyvault secret set \
  --vault-name sentinel-production-kv \
  --name virustotal-api-key \
  --value "your-api-key-here" \
  --expires "2025-12-31T23:59:59Z"  # Set expiration

# Set secret with content type and tags
az keyvault secret set \
  --vault-name sentinel-production-kv \
  --name database-connection-string \
  --value "Server=tcp:...;Database=...;" \
  --content-type "connection-string" \
  --tags environment=production owner=security-team
----

*Access control with RBAC*:

[,bash]
----
# Grant Logic App access to specific secrets
# Get Logic App managed identity
LOGIC_APP_IDENTITY=$(az logic app show \
  --name MalwareAnalysis-Playbook \
  --resource-group sentinel-rg \
  --query identity.principalId -o tsv)

# Grant Key Vault Secrets User role (read-only)
az role assignment create \
  --assignee $LOGIC_APP_IDENTITY \
  --role "Key Vault Secrets User" \
  --scope "/subscriptions/{sub}/resourceGroups/sentinel-rg/providers/Microsoft.KeyVault/vaults/sentinel-production-kv/secrets/virustotal-api-key"

# Grant Function App broader access
FUNCTION_IDENTITY=$(az functionapp identity show \
  --name sentinel-functions \
  --resource-group sentinel-rg \
  --query principalId -o tsv)

az role assignment create \
  --assignee $FUNCTION_IDENTITY \
  --role "Key Vault Secrets User" \
  --scope "/subscriptions/{sub}/resourceGroups/sentinel-rg/providers/Microsoft.KeyVault/vaults/sentinel-production-kv"
----

*Using Key Vault in Logic Apps*:

*Method 1: Key Vault connector* (recommended):

[,json]
----
{
  "type": "ApiConnection",
  "inputs": {
    "host": {
      "connection": {
        "name": "@parameters('$connections')['keyvault']['connectionId']"
      }
    },
    "method": "get",
    "path": "/secrets/@{encodeURIComponent('virustotal-api-key')}/value"
  },
  "runAfter": {},
  "runtimeConfiguration": {
    "secureData": {
      "properties": ["outputs"]  // Mark output as secure
    }
  }
}
----

Then use in subsequent action:

[,json]
----
{
  "type": "Http",
  "inputs": {
    "method": "GET",
    "uri": "https://www.virustotal.com/api/v3/ip_addresses/@{variables('IP')}",
    "headers": {
      "x-apikey": "@body('Get_VirusTotal_Key')?['value']"
    }
  },
  "runAfter": {
    "Get_VirusTotal_Key": ["Succeeded"]
  }
}
----

*Method 2: Direct URI reference* (for simple scenarios):

[,json]
----
// In Logic App parameters
{
  "VT_API_Key": {
    "type": "securestring",
    "value": "@keyvault('https://sentinel-production-kv.vault.azure.net/secrets/virustotal-api-key')"
  }
}

// Use in actions
{
  "headers": {
    "x-apikey": "@parameters('VT_API_Key')"
  }
}
----

*Using Key Vault in Azure Functions*:

[,python]
----
import os
from azure.identity import DefaultAzureCredential
from azure.keyvault.secrets import SecretClient
import functools
import logging

# Cache for secrets (reduces Key Vault calls)
_secret_cache = {}

def get_secret(secret_name: str, use_cache: bool = True) -> str:
    """
    Retrieve secret from Key Vault using managed identity

    Args:
        secret_name: Name of the secret in Key Vault
        use_cache: Whether to cache the secret (default True)

    Returns:
        Secret value as string
    """
    # Check cache first
    if use_cache and secret_name in _secret_cache:
        return _secret_cache[secret_name]

    try:
        # Get Key Vault URL from environment
        kv_url = os.environ.get('KEY_VAULT_URL')
        if not kv_url:
            raise ValueError("KEY_VAULT_URL environment variable not set")

        # Use managed identity
        credential = DefaultAzureCredential()
        client = SecretClient(vault_url=kv_url, credential=credential)

        # Retrieve secret
        secret = client.get_secret(secret_name)

        # Cache if enabled
        if use_cache:
            _secret_cache[secret_name] = secret.value

        logging.info(f"Retrieved secret: {secret_name}")
        return secret.value

    except Exception as e:
        logging.error(f"Failed to retrieve secret {secret_name}: {str(e)}")
        raise

# Decorator for functions needing secrets
def requires_secrets(*secret_names):
    """Decorator to inject secrets as function arguments"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # Retrieve all required secrets
            secrets = {name: get_secret(name) for name in secret_names}
            # Add to kwargs
            kwargs.update(secrets)
            return func(*args, **kwargs)
        return wrapper
    return decorator

# Usage example
@requires_secrets('virustotal-api-key', 'abuseipdb-api-key')
def enrich_ip_address(ip_address: str, **secrets):
    """
    Enrich IP address with threat intelligence
    Secrets automatically injected by decorator
    """
    vt_key = secrets['virustotal-api-key']
    abuse_key = secrets['abuseipdb-api-key']

    # Use keys for API calls
    vt_data = query_virustotal(ip_address, vt_key)
    abuse_data = query_abuseipdb(ip_address, abuse_key)

    return {
        'virustotal': vt_data,
        'abuseipdb': abuse_data
    }
----

*Rotation and lifecycle management*:

*Automated secret rotation*:

[,python]
----
import azure.functions as func
from azure.keyvault.secrets import SecretClient
from azure.identity import DefaultAzureCredential
from datetime import datetime, timedelta
import logging

def main(timer: func.TimerRequest) -> None:
    """
    Scheduled function to check secret expiration and alert
    Runs daily
    """
    credential = DefaultAzureCredential()
    kv_url = os.environ['KEY_VAULT_URL']
    client = SecretClient(vault_url=kv_url, credential=credential)

    # Get all secrets
    secrets = client.list_properties_of_secrets()

    expiring_soon = []
    expired = []

    for secret in secrets:
        if secret.expires_on:
            days_until_expiry = (secret.expires_on - datetime.now()).days

            if days_until_expiry < 0:
                expired.append({
                    'name': secret.name,
                    'expired_days_ago': abs(days_until_expiry)
                })
            elif days_until_expiry < 30:
                expiring_soon.append({
                    'name': secret.name,
                    'days_until_expiry': days_until_expiry
                })

    # Alert on expired secrets
    if expired:
        logging.error(f"EXPIRED SECRETS: {expired}")
        send_pagerduty_alert("Expired Key Vault secrets", expired)

    # Warn on soon-to-expire secrets
    if expiring_soon:
        logging.warning(f"Secrets expiring soon: {expiring_soon}")
        send_teams_notification("Secrets expiring in 30 days", expiring_soon)
----

*Secret versioning*:

[,bash]
----
# Update secret (creates new version, old version retained)
az keyvault secret set \
  --vault-name sentinel-production-kv \
  --name virustotal-api-key \
  --value "new-api-key-after-rotation"

# List all versions
az keyvault secret list-versions \
  --vault-name sentinel-production-kv \
  --name virustotal-api-key

# Get specific version
az keyvault secret show \
  --vault-name sentinel-production-kv \
  --name virustotal-api-key \
  --version abc123def456

# Disable old version (don't delete for audit trail)
az keyvault secret set-attributes \
  --vault-name sentinel-production-kv \
  --name virustotal-api-key \
  --version abc123def456 \
  --enabled false
----

*Monitoring and auditing*:

*Query Key Vault access logs in Sentinel*:

[,kusto]
----
// Monitor Key Vault secret access
AzureDiagnostics
| where ResourceType == "VAULTS"
| where OperationName == "SecretGet"
| where TimeGenerated > ago(24h)
| summarize AccessCount = count() by
    CallerIPAddress,
    identity_claim_upn_s,
    id_s  // Secret name
| order by AccessCount desc

// Alert on unusual access patterns
AzureDiagnostics
| where ResourceType == "VAULTS"
| where OperationName == "SecretGet"
| where TimeGenerated > ago(1h)
| summarize
    UniqueSecrets = dcount(id_s),
    AccessCount = count()
    by CallerIPAddress, identity_claim_upn_s
| where UniqueSecrets > 10 or AccessCount > 100  // Suspicious
| project
    TimeGenerated = now(),
    Caller = identity_claim_upn_s,
    IPAddress = CallerIPAddress,
    SecretsAccessed = UniqueSecrets,
    TotalAccesses = AccessCount
----

*Analytics rule for Key Vault anomalies*:

[,kusto]
----
// Detect after-hours Key Vault access
let BusinessHours = dynamic(["09", "10", "11", "12", "13", "14", "15", "16", "17"]);
AzureDiagnostics
| where ResourceType == "VAULTS"
| where OperationName in ("SecretGet", "SecretSet", "SecretDelete")
| where TimeGenerated > ago(1h)
| extend Hour = format_datetime(TimeGenerated, "HH")
| where Hour !in (BusinessHours)
| extend
    User = identity_claim_upn_s,
    IP = CallerIPAddress,
    Secret = id_s,
    Operation = OperationName
| summarize
    Operations = make_list(Operation),
    Secrets = make_set(Secret),
    count()
    by User, IP
| where array_length(Secrets) > 5  // Accessed multiple secrets
----

*Network security*:

*Private endpoint for Key Vault*:

[,bash]
----
# Disable public access
az keyvault update \
  --name sentinel-production-kv \
  --resource-group sentinel-rg \
  --default-action Deny \
  --bypass AzureServices  # Allow Azure services

# Create private endpoint
az network private-endpoint create \
  --name kv-private-endpoint \
  --resource-group sentinel-rg \
  --vnet-name security-vnet \
  --subnet private-endpoints-subnet \
  --private-connection-resource-id "/subscriptions/{sub}/resourceGroups/sentinel-rg/providers/Microsoft.KeyVault/vaults/sentinel-production-kv" \
  --group-id vault \
  --connection-name kv-connection

# Create private DNS zone
az network private-dns zone create \
  --resource-group sentinel-rg \
  --name privatelink.vaultcore.azure.net

# Link to VNet
az network private-dns link vnet create \
  --resource-group sentinel-rg \
  --zone-name privatelink.vaultcore.azure.net \
  --name kv-dns-link \
  --virtual-network security-vnet \
  --registration-enabled false
----

*Firewall rules* (if not using private endpoint):

[,bash]
----
# Allow specific IP ranges
az keyvault network-rule add \
  --name sentinel-production-kv \
  --resource-group sentinel-rg \
  --ip-address 203.0.113.0/24  # Office network

# Allow specific VNets
az keyvault network-rule add \
  --name sentinel-production-kv \
  --resource-group sentinel-rg \
  --vnet-name security-vnet \
  --subnet automation-subnet
----

*Best practices*:

* Always use managed identity (never service principals with secrets)
* Enable soft-delete and purge protection
* Set expiration dates on all secrets
* Implement secret rotation procedures
* Monitor and alert on Key Vault access
* Use private endpoints in production
* Separate Key Vaults per environment (dev/staging/prod)
* Tag secrets with owner and purpose
* Regular access reviews
* Audit logs integrated with Sentinel
* Cache secrets in Functions (reduce API calls and latency)
* Never log secret values
* Disable public network access when possible

Key Vault is critical infrastructure for Sentinel security - proper configuration and monitoring ensures secrets remain protected while enabling automated security operations.

== What are some advanced detection techniques you can implement in Microsoft Sentinel using KQL and analytics rules?

Advanced detection in Sentinel combines sophisticated KQL queries, behavioral analytics, threat intelligence, and machine learning to identify complex attack patterns that evade traditional signature-based detection.

*Advanced detection categories*:

*1. Behavioral anomaly detection* - identify deviations from normal patterns

*Example: Impossible travel detection*:

[,kusto]
----
// Detect user sign-ins from geographically impossible locations
let TimeWindow = 1h;
let SpeedThreshold = 800; // km/h (impossible for commercial travel)

SigninLogs
| where TimeGenerated > ago(1d)
| where ResultType == 0  // Successful sign-ins only
| project
    TimeGenerated,
    UserPrincipalName,
    IPAddress,
    Location,
    Latitude = toreal(LocationDetails.geoCoordinates.latitude),
    Longitude = toreal(LocationDetails.geoCoordinates.longitude)
| where isnotnull(Latitude) and isnotnull(Longitude)
| order by UserPrincipalName, TimeGenerated asc
| serialize
| extend
    PrevTime = prev(TimeGenerated, 1),
    PrevLat = prev(Latitude, 1),
    PrevLon = prev(Longitude, 1),
    PrevLocation = prev(Location, 1),
    PrevUser = prev(UserPrincipalName, 1)
| where UserPrincipalName == PrevUser  // Same user
| extend TimeDiffMinutes = datetime_diff('minute', TimeGenerated, PrevTime)
| where TimeDiffMinutes > 0 and TimeDiffMinutes <= 60  // Within time window
| extend DistanceKm = geo_distance_2points(PrevLon, PrevLat, Longitude, Latitude) / 1000
| extend SpeedKmH = DistanceKm / (TimeDiffMinutes / 60.0)
| where SpeedKmH > SpeedThreshold
| project
    TimeGenerated,
    User = UserPrincipalName,
    FirstLocation = PrevLocation,
    SecondLocation = Location,
    DistanceKm = round(DistanceKm, 2),
    TimeDiffMinutes,
    SpeedKmH = round(SpeedKmH, 2),
    IPAddress
| extend
    AlertSeverity = case(
        SpeedKmH > 2000, "High",
        SpeedKmH > 1500, "Medium",
        "Low"
    )
----

*Example: Abnormal data access pattern*:

[,kusto]
----
// Detect users accessing significantly more files than their baseline
let LookbackPeriod = 30d;
let AnalysisPeriod = 1h;
let AnomalyThreshold = 3.0;  // Standard deviations

// Calculate baseline (30-day average)
let Baseline =
    OfficeActivity
    | where TimeGenerated between (ago(LookbackPeriod) .. ago(AnalysisPeriod))
    | where Operation in ("FileAccessed", "FileDownloaded")
    | summarize
        AvgAccess = avg(FileAccessCount),
        StdDev = stdev(FileAccessCount)
        by UserId, bin(TimeGenerated, 1h)
    | summarize
        BaselineAvg = avg(AvgAccess),
        BaselineStdDev = avg(StdDev)
        by UserId;

// Current activity
let CurrentActivity =
    OfficeActivity
    | where TimeGenerated > ago(AnalysisPeriod)
    | where Operation in ("FileAccessed", "FileDownloaded")
    | summarize FileAccessCount = count() by UserId;

// Compare current to baseline
CurrentActivity
| join kind=inner (Baseline) on UserId
| extend ZScore = (FileAccessCount - BaselineAvg) / BaselineStdDev
| where ZScore > AnomalyThreshold
| project
    User = UserId,
    CurrentAccess = FileAccessCount,
    NormalBaseline = round(BaselineAvg, 2),
    DeviationScore = round(ZScore, 2),
    PercentIncrease = round(((FileAccessCount - BaselineAvg) / BaselineAvg) * 100, 2)
| order by DeviationScore desc
----

*2. Attack chain detection* - correlate multiple events identifying attack progression

*Example: Credential access â†’ Lateral movement â†’ Data exfiltration*:

[,kusto]
----
// Multi-stage attack detection
let TimeWindow = 4h;

// Stage 1: Credential access (password spray)
let PasswordSpray =
    SigninLogs
    | where TimeGenerated > ago(TimeWindow)
    | where ResultType != 0  // Failed
    | summarize
        FailedAttempts = count(),
        TargetedUsers = dcount(UserPrincipalName),
        UniqueIPs = make_set(IPAddress)
        by SourceIP = IPAddress
    | where FailedAttempts > 50 and TargetedUsers > 10
    | project SourceIP, PasswordSprayTime = now(), UniqueIPs;

// Stage 2: Successful compromise
let CompromisedAccounts =
    SigninLogs
    | where TimeGenerated > ago(TimeWindow)
    | where ResultType == 0  // Successful
    | join kind=inner (PasswordSpray) on $left.IPAddress == $right.SourceIP
    | where TimeGenerated > PasswordSprayTime
    | distinct UserPrincipalName, CompromiseTime = TimeGenerated, CompromiseIP = IPAddress;

// Stage 3: Lateral movement (RDP/SMB to other hosts)
let LateralMovement =
    SecurityEvent
    | where TimeGenerated > ago(TimeWindow)
    | where EventID in (4624, 4625)  // Logon events
    | where LogonType in (3, 10)  // Network or RDP
    | project TimeGenerated, Account, TargetHost = Computer, SourceIP = IpAddress
    | join kind=inner (CompromisedAccounts) on $left.Account == $right.UserPrincipalName
    | where TimeGenerated > CompromiseTime
    | summarize
        LateralTargets = make_set(TargetHost),
        LateralCount = count()
        by Account, CompromiseTime;

// Stage 4: Data exfiltration (large outbound transfers)
let DataExfiltration =
    AzureNetworkAnalytics_CL
    | where TimeGenerated > ago(TimeWindow)
    | where FlowDirection_s == "O"  // Outbound
    | summarize TotalBytes = sum(OutboundBytes_d) by SourceIP, DestinationIP
    | where TotalBytes > 1000000000  // > 1GB
    | project SourceIP, DestinationIP, ExfiltratedGB = TotalBytes / 1000000000;

// Correlate all stages
CompromisedAccounts
| join kind=inner (LateralMovement) on $left.UserPrincipalName == $right.Account
| join kind=inner (DataExfiltration) on $left.CompromiseIP == $right.SourceIP
| project
    AttackTimeline = strcat(
        "1. Password Spray -> ",
        "2. Compromise (", CompromiseTime, ") -> ",
        "3. Lateral Movement (", LateralCount, " hosts) -> ",
        "4. Exfiltration (", round(ExfiltratedGB, 2), " GB)"
    ),
    CompromisedUser = UserPrincipalName,
    CompromiseIP,
    LateralTargets,
    ExfiltratedGB,
    Severity = "Critical"
----

*3. Threat intelligence correlation* - match IOCs against activity

*Example: Advanced TI matching with context*:

[,kusto]
----
// Match threat indicators with enrichment
let ThreatIntel =
    ThreatIntelligenceIndicator
    | where TimeGenerated > ago(7d)
    | where isnotempty(NetworkIP) or isnotempty(EmailSourceIpAddress) or isnotempty(NetworkDestinationIP)
    | where Active == true
    | extend IOC = coalesce(NetworkIP, EmailSourceIpAddress, NetworkDestinationIP)
    | project IOC, ThreatType, Description, Confidence, ExternalID;

// Check multiple data sources
let SignInMatches =
    SigninLogs
    | where TimeGenerated > ago(1d)
    | join kind=inner (ThreatIntel) on $left.IPAddress == $right.IOC
    | extend Source = "SignIn", Activity = "Authentication";

let FirewallMatches =
    AzureDiagnostics
    | where ResourceType == "AZUREFIREWALLS"
    | where TimeGenerated > ago(1d)
    | extend SourceIP = tostring(split(msg_s, ":")[0])
    | join kind=inner (ThreatIntel) on $left.SourceIP == $right.IOC
    | extend Source = "Firewall", Activity = msg_s;

let EmailMatches =
    EmailEvents
    | where TimeGenerated > ago(1d)
    | join kind=inner (ThreatIntel) on $left.SenderIPv4 == $right.IOC
    | extend Source = "Email", Activity = Subject;

// Union all matches
union SignInMatches, FirewallMatches, EmailMatches
| project
    TimeGenerated,
    Source,
    IOC,
    ThreatType,
    Confidence,
    Activity,
    Description,
    User = coalesce(UserPrincipalName, Account, RecipientEmailAddress)
| extend Priority = case(
    Confidence > 80 and ThreatType contains "malware", "Critical",
    Confidence > 60, "High",
    "Medium"
)
| order by TimeGenerated desc
----

*4. Machine learning-based detection* - statistical anomalies

*Example: Time-series anomaly detection*:

[,kusto]
----
// Detect anomalous spikes in failed logins
SigninLogs
| where TimeGenerated > ago(30d)
| where ResultType != 0
| make-series FailedLogins = count() default=0 on TimeGenerated
    from ago(30d) to now() step 1h
| extend (anomalies, score, baseline) = series_decompose_anomalies(FailedLogins, 1.5, -1, 'linefit')
| mv-expand TimeGenerated to typeof(datetime), FailedLogins to typeof(long),
    anomalies to typeof(double), score to typeof(double), baseline to typeof(long)
| where anomalies != 0  // Anomaly detected
| project
    TimeGenerated,
    FailedLogins,
    Baseline = baseline,
    AnomalyScore = score,
    DeviationPercent = round(((FailedLogins - baseline) * 100.0 / baseline), 2)
| where DeviationPercent > 200  // 200% increase from baseline
----

*5. Behavioral profiling* - detect deviation from user/entity baselines

*Example: Peer group analysis*:

[,kusto]
----
// Detect user behaving differently than peers
let AnalysisPeriod = 1h;
let LookbackPeriod = 30d;

// Define peer groups (same department)
let PeerGroups =
    IdentityInfo
    | distinct UserPrincipalName, Department
    | where isnotempty(Department);

// Calculate peer group baselines
let PeerBaseline =
    SigninLogs
    | where TimeGenerated between (ago(LookbackPeriod) .. ago(AnalysisPeriod))
    | join kind=inner (PeerGroups) on UserPrincipalName
    | summarize
        PeerAvgLogins = avg(LoginCount),
        PeerStdDev = stdev(LoginCount)
        by Department, bin(TimeGenerated, 1d)
    | summarize
        DeptAvgLogins = avg(PeerAvgLogins),
        DeptStdDev = avg(PeerStdDev)
        by Department;

// Current user activity
let CurrentActivity =
    SigninLogs
    | where TimeGenerated > ago(AnalysisPeriod)
    | summarize LoginCount = count() by UserPrincipalName
    | join kind=inner (PeerGroups) on UserPrincipalName;

// Compare to peer baseline
CurrentActivity
| join kind=inner (PeerBaseline) on Department
| extend DeviationScore = (LoginCount - DeptAvgLogins) / DeptStdDev
| where abs(DeviationScore) > 3  // 3 standard deviations
| project
    User = UserPrincipalName,
    Department,
    CurrentLogins = LoginCount,
    PeerAverage = round(DeptAvgLogins, 2),
    DeviationScore = round(DeviationScore, 2),
    BehaviorType = iff(DeviationScore > 0, "Excessive Activity", "Unusually Low Activity")
----

*6. Fusion detection* - multi-signal correlation

*Example: Combining weak signals into high-confidence alert*:

[,kusto]
----
// Combine multiple weak indicators into strong signal
let TimeWindow = 24h;

// Weak signal 1: Unusual login time
let UnusualTime =
    SigninLogs
    | where TimeGenerated > ago(TimeWindow)
    | extend Hour = hourofday(TimeGenerated)
    | where Hour < 6 or Hour > 22  // Outside business hours
    | distinct UserPrincipalName
    | extend UnusualTimeScore = 1;

// Weak signal 2: New device
let NewDevice =
    SigninLogs
    | where TimeGenerated > ago(TimeWindow)
    | extend DeviceId = tostring(DeviceDetail.deviceId)
    | join kind=leftanti (
        SigninLogs
        | where TimeGenerated between (ago(90d) .. ago(TimeWindow))
        | extend DeviceId = tostring(DeviceDetail.deviceId)
        | distinct UserPrincipalName, DeviceId
    ) on UserPrincipalName, DeviceId
    | distinct UserPrincipalName
    | extend NewDeviceScore = 1;

// Weak signal 3: Failed MFA
let FailedMFA =
    SigninLogs
    | where TimeGenerated > ago(TimeWindow)
    | where AuthenticationRequirement == "multiFactorAuthentication"
    | where ResultType != 0
    | distinct UserPrincipalName
    | extend FailedMFAScore = 1;

// Weak signal 4: Unusual location
let UnusualLocation =
    SigninLogs
    | where TimeGenerated > ago(TimeWindow)
    | extend Country = tostring(LocationDetails.countryOrRegion)
    | join kind=leftanti (
        SigninLogs
        | where TimeGenerated between (ago(90d) .. ago(TimeWindow))
        | extend Country = tostring(LocationDetails.countryOrRegion)
        | distinct UserPrincipalName, Country
    ) on UserPrincipalName, Country
    | distinct UserPrincipalName
    | extend UnusualLocationScore = 1;

// Weak signal 5: High-risk IP
let RiskyIP =
    SigninLogs
    | where TimeGenerated > ago(TimeWindow)
    | where RiskLevelDuringSignIn in ("high", "medium")
    | distinct UserPrincipalName
    | extend RiskyIPScore = 1;

// Combine all signals
UnusualTime
| join kind=fullouter (NewDevice) on UserPrincipalName
| join kind=fullouter (FailedMFA) on UserPrincipalName
| join kind=fullouter (UnusualLocation) on UserPrincipalName
| join kind=fullouter (RiskyIP) on UserPrincipalName
| extend
    User = coalesce(UserPrincipalName, UserPrincipalName1, UserPrincipalName2, UserPrincipalName3, UserPrincipalName4),
    RiskScore = coalesce(UnusualTimeScore, 0) +
                coalesce(NewDeviceScore, 0) +
                coalesce(FailedMFAScore, 0) +
                coalesce(UnusualLocationScore, 0) +
                coalesce(RiskyIPScore, 0)
| where RiskScore >= 3  // At least 3 weak signals = strong signal
| project
    User,
    RiskScore,
    Indicators = pack(
        "UnusualTime", coalesce(UnusualTimeScore, 0),
        "NewDevice", coalesce(NewDeviceScore, 0),
        "FailedMFA", coalesce(FailedMFAScore, 0),
        "UnusualLocation", coalesce(UnusualLocationScore, 0),
        "RiskyIP", coalesce(RiskyIPScore, 0)
    ),
    Severity = case(
        RiskScore >= 4, "High",
        RiskScore == 3, "Medium",
        "Low"
    )
| order by RiskScore desc
----

*7. Living-off-the-land detection* - identify abuse of legitimate tools

*Example: Detecting malicious PowerShell*:

[,kusto]
----
// Detect suspicious PowerShell usage
SecurityEvent
| where TimeGenerated > ago(1h)
| where EventID == 4688  // Process creation
| where Process has_any ("powershell.exe", "pwsh.exe")
| where CommandLine has_any (
    "-encodedcommand",  // Encoded commands (obfuscation)
    "-enc",
    "downloadstring",  // Download from internet
    "invoke-webrequest",
    "iwr",
    "net.webclient",
    "bitstransfer",  // BITS for downloads
    "-windowstyle hidden",  // Hidden execution
    "-noprofile",  // Bypass profiles
    "-noninteractive",
    "invoke-mimikatz",  // Credential dumping
    "invoke-expression",  // Code execution
    "iex",
    "bypass",  // Execution policy bypass
    "invoke-shellcode"  // Shellcode injection
)
| extend
    SuspiciousFlags = pack_array(
        iff(CommandLine contains "-encodedcommand" or CommandLine contains "-enc", "Encoded", ""),
        iff(CommandLine contains "downloadstring" or CommandLine contains "invoke-webrequest", "Download", ""),
        iff(CommandLine contains "-windowstyle hidden", "Hidden", ""),
        iff(CommandLine contains "invoke-mimikatz", "CredentialTheft", ""),
        iff(CommandLine contains "bypass", "PolicyBypass", "")
    ),
    RiskScore =
        iff(CommandLine contains "invoke-mimikatz", 50, 0) +
        iff(CommandLine contains "-encodedcommand", 30, 0) +
        iff(CommandLine contains "downloadstring", 20, 0) +
        iff(CommandLine contains "-windowstyle hidden", 10, 0) +
        iff(CommandLine contains "bypass", 10, 0)
| where array_length(SuspiciousFlags) > 1  // Multiple indicators
| project
    TimeGenerated,
    Computer,
    Account,
    CommandLine,
    SuspiciousFlags = array_strcat(SuspiciousFlags, ", "),
    RiskScore,
    ParentProcess = ParentProcessName
| order by RiskScore desc
----

*8. Supply chain attack detection*:

[,kusto]
----
// Detect compromised software updates
let TrustedUpdateServers = dynamic([
    "windowsupdate.microsoft.com",
    "update.microsoft.com",
    "download.windowsupdate.com"
]);

let UpdateProcesses = dynamic([
    "wuauclt.exe",
    "TrustedInstaller.exe",
    "WindowsUpdateHost.exe"
]);

// Detect updates from non-trusted sources
SecurityEvent
| where EventID == 4688
| where Process has_any (UpdateProcesses)
| extend ParentImage = tostring(split(ParentProcessName, "\\")[-1])
| where ParentImage !in (UpdateProcesses)  // Unexpected parent
| join kind=leftouter (
    DnsEvents
    | where TimeGenerated > ago(5m)
    | where QueryType == "A"
    | extend Domain = tolower(Name)
) on Computer
| where isnotempty(Domain)
| where not(Domain has_any (TrustedUpdateServers))
| project
    TimeGenerated,
    Computer,
    Account,
    SuspiciousProcess = Process,
    UnexpectedParent = ParentProcessName,
    UntrustedDomain = Domain,
    Severity = "High",
    Description = "Software update process communicating with untrusted domain - possible supply chain attack"
----

*Best practices for advanced detection*:

* Combine multiple detection techniques (behavioral + TI + ML)
* Tune thresholds based on environment baselines
* Regular review and refinement of detection logic
* Balance false positives vs. detection coverage
* Document detection logic and expected false positive rate
* A/B test new detection rules before production
* Monitor rule performance (execution time, results)
* Version control all analytics rules
* Peer review complex detection logic
* Test rules against historical data
* Gradually increase sensitivity as tuning improves

Advanced detection transforms Sentinel from reactive SIEM to proactive threat detection platform - enabling identification of sophisticated attacks that evade traditional signatures through behavioral analysis, correlation, and machine learning.
