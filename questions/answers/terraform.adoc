# Terraform Security Engineering Interview Answers

## Terraform – Core Concepts

### 1. What is Terraform, and what is its primary purpose?

Terraform is an open-source Infrastructure as Code (IaC) tool created by HashiCorp that enables you to define, provision, and manage infrastructure using declarative configuration files. **Primary purpose**: Automate infrastructure provisioning across multiple cloud providers and services through code, enabling version control, reproducibility, and collaboration. Instead of manually clicking through cloud consoles or writing imperative scripts, you declare desired infrastructure state in configuration files, and Terraform handles the execution. **Key characteristics**: **Declarative** - you specify what infrastructure you want (desired state), not how to create it (Terraform figures out steps). **Cloud-agnostic** - works with AWS, Azure, GCP, Kubernetes, and 1000+ providers. **State management** - tracks infrastructure state enabling updates and drift detection. **Plan before apply** - preview changes before execution reducing risks. **From security perspective**: Terraform enables infrastructure security at scale through consistent configuration enforcement, security controls defined as code and reviewed, audit trail via version control showing who changed what and when, automated compliance checking against security policies, and reproducible secure infrastructure across environments. Security teams can review infrastructure changes in pull requests before deployment, implement policy-as-code (Sentinel, OPA) blocking insecure configurations, and maintain golden modules with security best practices baked in. Terraform is foundational to modern cloud security because it treats infrastructure configuration as software - versioned, tested, reviewed, and deployed through controlled pipelines rather than ad-hoc manual changes.

### 2. What makes Terraform a cloud-agnostic tool?

Terraform achieves cloud-agnosticism through its **provider architecture** - a plugin system allowing Terraform to interact with different platforms via standardized APIs. **How it works**: **Provider plugins** - separate binaries for each platform (AWS, Azure, GCP, Kubernetes, etc.) downloaded automatically when running `terraform init`. **Common HCL syntax** - same configuration language regardless of provider enabling consistent experience across clouds. **State abstraction** - Terraform's state management works identically across all providers. **Benefits for organizations**: **Avoid vendor lock-in** - can migrate between clouds or use multiple clouds without rewriting entire infrastructure. **Consistent workflow** - same `terraform plan`, `terraform apply` commands regardless of target platform. **Unified tooling** - single tool for all infrastructure (cloud, SaaS, on-premises). **Skills transferability** - team learns Terraform once, applies everywhere.

**Example multi-cloud configuration**:
```hcl
# AWS resources
provider "aws" {
  region = "us-east-1"
}

resource "aws_s3_bucket" "data" {
  bucket = "myapp-data"
}

# Azure resources
provider "azurerm" {
  features {}
}

resource "azurerm_storage_account" "backup" {
  name                = "myappbackup"
  resource_group_name = azurerm_resource_group.main.name
}

# GCP resources
provider "google" {
  project = "my-project"
  region  = "us-central1"
}

resource "google_storage_bucket" "archive" {
  name     = "myapp-archive"
  location = "US"
}
```

**Security implications**: **Multi-cloud strategy** requires consistent security across providers (encryption, access controls, monitoring), Terraform enables defining security standards once applied everywhere, but cloud-specific security features require provider-specific configuration. **Caution**: Cloud-agnostic doesn't mean cloud-independent - each provider has unique security capabilities, Terraform abstracts provisioning but can't abstract away cloud differences, security teams must understand each cloud's security model. **Best practice**: Use Terraform's cloud-agnosticism for consistent provisioning workflow and security baseline but leverage cloud-native security features where appropriate through provider-specific resources.

### 3. How does Terraform differ from other IaC tools like CloudFormation or Ansible?

**Terraform vs CloudFormation**: **Cloud scope** - Terraform: multi-cloud and multi-platform; CloudFormation: AWS-only. **Language** - Terraform: HCL (human-readable); CloudFormation: JSON/YAML (verbose). **State management** - Terraform: explicit state file tracking resources; CloudFormation: implicit (AWS manages stacks). **Modularity** - Terraform: modules highly reusable across projects; CloudFormation: nested stacks (more complex). **Community** - Terraform: 1000+ providers, large community; CloudFormation: AWS resources only. **Import** - Terraform: can import existing resources; CloudFormation: limited import capability. **From security perspective**: Terraform provides better multi-cloud security management, CloudFormation tightly integrated with AWS security services (native IAM, KMS).

**Terraform vs Ansible**: **Paradigm** - Terraform: declarative (what you want); Ansible: primarily imperative (how to do it). **Purpose** - Terraform: infrastructure provisioning; Ansible: configuration management and orchestration. **State** - Terraform: stateful (tracks resources); Ansible: stateless (unless using Tower/AWX). **Idempotency** - Terraform: inherent (runs create desired state); Ansible: modules should be idempotent but must be designed carefully. **Mutable vs Immutable** - Terraform: encourages immutable infrastructure (replace, not modify); Ansible: typically modifies in-place (mutable). **Agent** - Terraform: agentless (API-driven); Ansible: agentless (SSH/WinRM).

**Comparison table**:
| Feature | Terraform | CloudFormation | Ansible |
|---------|-----------|----------------|---------|
| Scope | Multi-cloud | AWS only | Multi-cloud |
| Approach | Declarative | Declarative | Imperative/Declarative |
| State | Explicit state file | Managed by AWS | Stateless |
| Use case | Infrastructure provisioning | AWS infrastructure | Configuration + provisioning |
| Language | HCL | JSON/YAML | YAML |
| Modularity | Excellent | Good (nested stacks) | Good (roles) |
| Learning curve | Moderate | Moderate | Easy to start |

**When to use what**: **Terraform**: Multi-cloud infrastructure, immutable infrastructure patterns, team needs version-controlled infrastructure, strong state management required. **CloudFormation**: AWS-only shop, deep AWS integration needed, comfortable with AWS ecosystem. **Ansible**: Configuration management (OS setup, app deployment), existing infrastructure (mutable), orchestration workflows, simple automation tasks. **In practice, combined**: Terraform provisions infrastructure (VMs, networks, databases), Ansible configures instances (install software, configure services). **Security considerations**: **Terraform** - security controls defined declaratively, policy-as-code enforcement (Sentinel, OPA), state file security critical (contains sensitive data). **CloudFormation** - tight AWS IAM integration, AWS-native security features, CloudFormation service roles for deployment. **Ansible** - Ansible Vault for secrets, SSH key management important, playbook security review needed. Many organizations use Terraform for infrastructure provisioning and Ansible for configuration management - complementary tools serving different purposes.

### 4. What is the core Terraform workflow?

The core Terraform workflow consists of four stages: **Write → Plan → Apply → Repeat**.

**1. Write** - define infrastructure as code:
```hcl
# main.tf
resource "aws_instance" "web" {
  ami           = "ami-0c55b159cbfafe1f0"
  instance_type = "t3.micro"
  
  tags = {
    Name = "WebServer"
  }
}
```

**2. Initialize** (`terraform init`): Download provider plugins (AWS, Azure, etc.), initialize backend (state storage), download referenced modules, prepare working directory. Run once per directory or when adding new providers/modules.

**3. Plan** (`terraform plan`): Compare desired state (configuration files) with current state (state file), determine what actions needed (create, update, delete), show execution plan for review, no changes made yet (dry-run). **Critical security gate** - review plan before applying, verify no unintended changes, check for security implications.

**4. Apply** (`terraform apply`): Execute the plan creating/modifying/deleting resources, update state file with current infrastructure, output results and any output values. Changes infrastructure to match configuration.

**5. Destroy** (`terraform destroy`) when needed: Remove all managed infrastructure, clean up resources, update state file. Only when decommissioning.

**Complete workflow example**:
```bash
# 1. Write configuration (main.tf created)

# 2. Initialize
terraform init
# Output: Initializing provider plugins...

# 3. Plan and review
terraform plan -out=tfplan
# Output: Plan: 1 to add, 0 to change, 0 to destroy

# Review plan carefully!
# Check for unexpected changes
# Verify security configurations

# 4. Apply
terraform apply tfplan
# Infrastructure created

# 5. Make changes to config, repeat plan/apply
# Edit main.tf...
terraform plan
terraform apply

# 6. When done, destroy
terraform destroy
```

**Security workflow enhancements**: **Pre-commit** - lint configuration (tflint), scan for security issues (tfsec, checkov), detect secrets (git-secrets). **Plan stage** - automated security scanning of plan, policy validation (Sentinel, OPA), peer review of plan output. **Apply stage** - approval gate for production, deployment only from CI/CD (not local), state file backup before apply. **Post-apply** - verify deployment, compliance scanning, drift detection scheduled. **Best practices**: Always run `plan` before `apply`, save plan output for approval workflow (`terraform plan -out=tfplan`), never skip plan in automation, implement approval gates for production, comprehensive logging and audit trail, and version control all configuration files. The workflow is iterative - write, plan, review, apply, repeat - with security checkpoints at each stage ensuring safe infrastructure changes.

### 5. What are the key Terraform commands, and what do they do?

**Core commands**:

**`terraform init`** - Initialize working directory: downloads provider plugins, initializes backend, downloads modules. Run first in new directory or when adding providers/modules. Idempotent (safe to run multiple times).

**`terraform plan`** - Preview changes: compares config with state, shows what will be created/modified/deleted, saves plan to file with `-out`. Always run before apply. No infrastructure changes.

**`terraform apply`** - Apply changes: executes plan creating/updating/deleting resources, updates state file. Can auto-approve with `-auto-approve` (use cautiously). Modifies infrastructure.

**`terraform destroy`** - Destroy infrastructure: removes all managed resources, updates state file. Requires confirmation. Use with extreme caution.

**State management**:

**`terraform state list`** - List resources in state: shows all managed resources. Useful for inventory.

**`terraform state show <resource>`** - Show resource details: displays attributes of specific resource from state.

**`terraform state mv`** - Move resource in state: renames resource in state without destroying/recreating. Useful for refactoring.

**`terraform state rm`** - Remove resource from state: stops Terraform managing resource (doesn't delete resource). Use when manually deleting or transferring management.

**Validation and formatting**:

**`terraform validate`** - Validate configuration: checks syntax and internal consistency. Doesn't check provider APIs. Quick syntax check.

**`terraform fmt`** - Format configuration: standardizes formatting (indentation, spacing). Run before committing.

**Import and refresh**:

**`terraform import`** - Import existing resource: add existing infrastructure to Terraform management. Requires resource address and ID.

**`terraform refresh`** - Update state from real infrastructure: sync state with actual resources. (Deprecated - plan now does this automatically).

**Workspace management**:

**`terraform workspace list/new/select`** - Manage workspaces: separate state files for different environments.

**Advanced**:

**`terraform taint <resource>`** - Mark resource for recreation: forces resource destruction and recreation on next apply. (Deprecated - use `terraform apply -replace=<resource>`).

**`terraform output`** - Show outputs: display output values from state.

**`terraform graph`** - Generate dependency graph: visualize resource dependencies (DOT format).

**`terraform show`** - Inspect state or plan: human-readable output of state or saved plan.

**Security-relevant commands**:

**`terraform plan -out=tfplan`** - Save plan for approval: separates planning from application enabling review gates.

**`terraform show -json tfplan`** - JSON plan output: machine-readable format for automated security scanning.

**`terraform state pull`** - Download remote state: for backup or inspection (carefully - contains sensitive data).

**`terraform providers lock`** - Lock provider versions: creates dependency lock file preventing supply chain attacks.

**Example secure workflow**:
```bash
# Format and validate
terraform fmt -recursive
terraform validate

# Security scanning (external tools)
tfsec .
checkov -d .

# Plan with approval
terraform plan -out=tfplan

# Review plan
terraform show tfplan

# Apply specific plan (approved)
terraform apply tfplan

# Verify outputs
terraform output

# Check state
terraform state list
```

**Best practices**: Never use `-auto-approve` in production without review, always save and review plans before apply, use `terraform fmt` before commits, run `terraform validate` in CI/CD, implement policy-as-code scanning plan output, lock provider versions for reproducibility, regularly backup state files, and audit terraform commands in production (comprehensive logging). Understanding these commands is essential for safe Terraform operations - misuse can cause production outages or security issues.

### 6. What is the basic structure of a Terraform configuration file?

Terraform configurations written in HashiCorp Configuration Language (HCL) with `.tf` extension. Basic structure includes several key components:

**Provider block** - specifies which provider (AWS, Azure, etc.):
```hcl
provider "aws" {
  region = "us-east-1"
}
```

**Resource block** - defines infrastructure components:
```hcl
resource "aws_instance" "web_server" {
  ami           = "ami-0c55b159cbfafe1f0"
  instance_type = "t3.micro"
  
  tags = {
    Name = "WebServer"
  }
}
```

**Data source block** - query existing infrastructure:
```hcl
data "aws_ami" "ubuntu" {
  most_recent = true
  owners      = ["099720109477"]  # Canonical
  
  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"]
  }
}
```

**Variable block** - define inputs:
```hcl
variable "instance_type" {
  description = "EC2 instance type"
  type        = string
  default     = "t3.micro"
}
```

**Output block** - expose values:
```hcl
output "instance_ip" {
  description = "Public IP of web server"
  value       = aws_instance.web_server.public_ip
}
```

**Locals block** - define local values:
```hcl
locals {
  common_tags = {
    Environment = "production"
    ManagedBy   = "terraform"
  }
}
```

**Module block** - call reusable modules:
```hcl
module "vpc" {
  source = "./modules/vpc"
  
  cidr_block = "10.0.0.0/16"
  environment = "prod"
}
```

**Complete example** with security best practices:
```hcl
# versions.tf - specify required versions
terraform {
  required_version = ">= 1.0"
  
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 4.0"
    }
  }
  
  # Remote state with encryption
  backend "s3" {
    bucket         = "terraform-state-bucket"
    key            = "prod/terraform.tfstate"
    region         = "us-east-1"
    encrypt        = true
    dynamodb_table = "terraform-locks"
  }
}

# providers.tf
provider "aws" {
  region = var.aws_region
  
  default_tags {
    tags = local.common_tags
  }
}

# variables.tf
variable "aws_region" {
  description = "AWS region"
  type        = string
  default     = "us-east-1"
}

variable "instance_type" {
  description = "EC2 instance type"
  type        = string
  default     = "t3.micro"
}

# locals.tf
locals {
  common_tags = {
    Environment = var.environment
    Project     = "web-app"
    ManagedBy   = "terraform"
    Owner       = "platform-team"
  }
}

# main.tf
data "aws_ami" "ubuntu" {
  most_recent = true
  owners      = ["099720109477"]
  
  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"]
  }
}

resource "aws_instance" "web" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = var.instance_type
  
  # Security: no public IP, private subnet
  associate_public_ip_address = false
  subnet_id                   = module.vpc.private_subnet_ids[0]
  
  # Security: instance profile with minimal permissions
  iam_instance_profile = aws_iam_instance_profile.web.name
  
  # Security: encrypted EBS
  root_block_device {
    encrypted = true
  }
  
  # Security: security group limiting access
  vpc_security_group_ids = [aws_security_group.web.id]
  
  tags = merge(local.common_tags, {
    Name = "web-server"
  })
}

# outputs.tf
output "instance_id" {
  description = "EC2 instance ID"
  value       = aws_instance.web.id
}

output "private_ip" {
  description = "Private IP address"
  value       = aws_instance.web.private_ip
  sensitive   = false  # Not sensitive (private IP)
}
```

**File organization best practices**: **Single directory structure**: `main.tf` - primary resources, `variables.tf` - input variables, `outputs.tf` - output values, `providers.tf` - provider config, `versions.tf` - version constraints, `locals.tf` - local values, `data.tf` - data sources. **Or resource-focused**: `ec2.tf`, `rds.tf`, `vpc.tf` - grouped by resource type.

**Security considerations**: **Never commit secrets** - use variables from environment or secret managers. **Sensitive outputs** - mark with `sensitive = true` to prevent logging. **Version constraints** - pin provider versions preventing unexpected changes. **State backend** - always use encrypted remote backend. **Resource naming** - consistent naming conventions for audit trails. **Tags** - comprehensive tagging for security, compliance, cost tracking.

### 7. What are Terraform providers, and why are they important?

Providers are Terraform plugins enabling interaction with APIs of cloud platforms, SaaS providers, and other services. They translate HCL configuration into API calls creating/managing resources.

**What providers do**: **API abstraction** - provide unified interface to disparate APIs, handle authentication and API specifics, manage API rate limiting and retries. **Resource types** - define available resource types (aws_instance, azure_vm), specify resource arguments and attributes. **Data sources** - enable querying existing infrastructure. **State mapping** - map Terraform resources to API resources for state tracking.

**Provider examples**: **Cloud providers**: AWS (aws), Azure (azurerm), Google Cloud (google), DigitalOcean (digitalocean). **Kubernetes**: kubernetes, helm. **SaaS**: Datadog (datadog), PagerDuty (pagerduty), GitHub (github). **Infrastructure**: vSphere (vsphere), Docker (docker). **Security**: Vault (vault), Auth0 (auth0).

**Provider configuration**:
```hcl
# AWS provider
provider "aws" {
  region = "us-east-1"
  
  # Security: use IAM role, not hardcoded keys
  # Credentials from environment, instance profile, or AWS config
  
  default_tags {
    tags = {
      ManagedBy = "terraform"
    }
  }
}

# Azure provider
provider "azurerm" {
  features {}  # Required
  
  subscription_id = var.azure_subscription_id
}

# Multiple provider instances (different regions)
provider "aws" {
  alias  = "us_west"
  region = "us-west-2"
}

resource "aws_instance" "west" {
  provider = aws.us_west  # Use specific provider
  
  ami           = "ami-xyz"
  instance_type = "t3.micro"
}
```

**Provider versions**:
```hcl
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"  # Registry source
      version = "~> 4.0"  # Version constraint
    }
  }
}
```

**Why providers are important**: **Extensibility** - 1000+ providers support virtually any service, community can create custom providers. **Abstraction** - consistent workflow across different platforms, same HCL syntax regardless of provider. **Updates** - providers updated independently of Terraform core, bug fixes and new features without Terraform upgrade. **Security and compliance** - providers implement platform-specific security, handle authentication properly, support compliance features.

**Security considerations for providers**: **Authentication** - **Never hardcode credentials** in provider blocks, use environment variables, instance profiles/managed identities, credential files. **Example secure auth**:
```hcl
provider "aws" {
  region = "us-east-1"
  # Credentials automatically from:
  # 1. Environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
  # 2. Shared credentials file (~/.aws/credentials)
  # 3. IAM instance profile (EC2)
  # 4. IAM role (ECS task, Lambda)
}
```

**Provider versioning** - **Lock provider versions** preventing supply chain attacks, use version constraints (~> for minor updates, = for exact version). **Example**:
```hcl
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "= 4.67.0"  # Exact version for security
    }
  }
}
```

**Dependency lock file** - `terraform init` creates `.terraform.lock.hcl` locking exact provider versions and checksums, commit to version control for team consistency, prevents malicious provider substitution.

**Provider trust** - Providers are executable binaries (security risk), use official providers from HashiCorp registry (verified), audit custom/community providers before use, and review provider source code if highly sensitive.

**Best practices**: Explicitly define provider versions, commit `.terraform.lock.hcl` to version control, use official verified providers when possible, implement least privilege for provider credentials, never commit provider credentials to Git, use separate providers for different environments (dev/prod), regularly update providers for security patches, and audit provider permissions and access. Providers are the interface between Terraform and your infrastructure - securing them is critical for overall security posture.

### 8. What are Terraform resources?

Resources are the fundamental building blocks in Terraform representing infrastructure components like VMs, networks, databases, or any manageable entity.

**Resource syntax**:
```hcl
resource "resource_type" "resource_name" {
  # Configuration arguments
  argument1 = "value1"
  argument2 = "value2"
}
```

**Example resources**:
```hcl
# AWS EC2 instance
resource "aws_instance" "web" {
  ami           = "ami-0c55b159cbfafe1f0"
  instance_type = "t3.micro"
  
  tags = {
    Name = "WebServer"
  }
}

# AWS S3 bucket
resource "aws_s3_bucket" "data" {
  bucket = "my-data-bucket"
}

# Azure virtual machine
resource "azurerm_linux_virtual_machine" "main" {
  name                = "myvm"
  resource_group_name = azurerm_resource_group.main.name
  location            = azurerm_resource_group.main.location
  size                = "Standard_B2s"
  
  admin_username = "adminuser"
}
```

**Resource characteristics**: **Unique identifier** - resource type + name uniquely identifies resource in configuration. **State tracking** - Terraform tracks each resource in state file. **Lifecycle** - resources created, updated, or destroyed based on configuration changes. **Dependencies** - implicit or explicit relationships with other resources. **Attributes** - computed values after creation (IDs, IPs, etc.).

**Resource lifecycle**:
```hcl
resource "aws_instance" "example" {
  ami           = "ami-xyz"
  instance_type = "t3.micro"
  
  lifecycle {
    create_before_destroy = true  # Create new before deleting old
    prevent_destroy       = true  # Prevent accidental deletion
    ignore_changes        = [tags]  # Ignore specific changes
  }
}
```

**Resource meta-arguments**: **`depends_on`** - explicit dependencies. **`count`** - create multiple identical resources. **`for_each`** - create resources from map/set. **`provider`** - specify which provider to use. **`lifecycle`** - control resource lifecycle. **`provisioner`** - execute scripts during creation/destruction (discouraged).

**Resource references**: Resources can reference each other creating dependencies:
```hcl
resource "aws_instance" "web" {
  ami             = data.aws_ami.ubuntu.id
  instance_type   = "t3.micro"
  subnet_id       = aws_subnet.main.id  # Reference subnet
  security_groups = [aws_security_group.web.id]  # Reference SG
}

# Can access attributes
output "instance_ip" {
  value = aws_instance.web.public_ip  # Access computed attribute
}
```

**Security-focused resource examples**:
```hcl
# Encrypted S3 bucket
resource "aws_s3_bucket" "secure" {
  bucket = "secure-data-bucket"
}

resource "aws_s3_bucket_server_side_encryption_configuration" "secure" {
  bucket = aws_s3_bucket.secure.id
  
  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm     = "aws:kms"
      kms_master_key_id = aws_kms_key.s3.arn
    }
  }
}

resource "aws_s3_bucket_public_access_block" "secure" {
  bucket = aws_s3_bucket.secure.id
  
  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# Security group with least privilege
resource "aws_security_group" "web" {
  name        = "web-sg"
  description = "Web server security group"
  vpc_id      = aws_vpc.main.id
  
  # Only allow HTTPS
  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  
  # Minimal outbound
  egress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  
  tags = {
    Name = "web-sg"
  }
}

# IAM role with least privilege
resource "aws_iam_role" "ec2_role" {
  name = "ec2-app-role"
  
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Principal = {
        Service = "ec2.amazonaws.com"
      }
      Action = "sts:AssumeRole"
    }]
  })
}

resource "aws_iam_role_policy" "ec2_policy" {
  name = "ec2-policy"
  role = aws_iam_role.ec2_role.id
  
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Action = [
        "s3:GetObject"  # Only read from specific bucket
      ]
      Resource = "${aws_s3_bucket.data.arn}/*"
    }]
  })
}
```

**Resource naming conventions**: Use descriptive names (not "resource1"), follow consistent pattern (resource_type_purpose), lowercase with underscores. **Example**: `aws_instance.web_server`, `aws_security_group.database_sg`, `azurerm_storage_account.app_storage`.

**Best practices**: **Least privilege** - grant minimum necessary permissions. **Encryption** - enable encryption by default on all resources supporting it. **Access control** - restrict access through security groups, IAM policies, network ACLs. **Tagging** - comprehensive tags for security, compliance, cost tracking. **Immutable infrastructure** - prefer replacing resources over modifying (create_before_destroy). **Validation** - use lifecycle rules preventing accidental destructive changes. Resources are the actual infrastructure Terraform manages - properly securing them is essential for overall infrastructure security.

### 9. What are Terraform modules, and why are modules used?

Modules are containers for multiple resources that are used together, enabling reusable, composable infrastructure components. A module is simply a directory containing `.tf` files.

**Why use modules**: **Reusability** - write once, use many times across projects. **Consistency** - standardized configurations ensuring best practices. **Abstraction** - hide complexity behind simple interface. **Organization** - logical grouping of related resources. **Testing** - modules can be tested independently. **Security enforcement** - embed security controls in modules.

**Module structure**:
```
modules/
  secure-vpc/
    main.tf        # Resources
    variables.tf   # Inputs
    outputs.tf     # Outputs
    README.md      # Documentation
```

**Example secure VPC module**:
```hcl
# modules/secure-vpc/variables.tf
variable "vpc_cidr" {
  description = "CIDR block for VPC"
  type        = string
}

variable "environment" {
  description = "Environment name"
  type        = string
}

# modules/secure-vpc/main.tf
resource "aws_vpc" "main" {
  cidr_block           = var.vpc_cidr
  enable_dns_hostnames = true
  enable_dns_support   = true
  
  tags = {
    Name        = "${var.environment}-vpc"
    Environment = var.environment
  }
}

resource "aws_flow_log" "main" {
  vpc_id          = aws_vpc.main.id
  traffic_type    = "ALL"
  iam_role_arn    = aws_iam_role.flow_logs.arn
  log_destination = aws_cloudwatch_log_group.flow_logs.arn
}

resource "aws_default_security_group" "default" {
  vpc_id = aws_vpc.main.id
  
  # Lock down default SG (best practice)
  # No ingress/egress rules = deny all
  
  tags = {
    Name = "${var.environment}-default-sg-locked"
  }
}

# modules/secure-vpc/outputs.tf
output "vpc_id" {
  description = "VPC ID"
  value       = aws_vpc.main.id
}

output "vpc_cidr" {
  description = "VPC CIDR block"
  value       = aws_vpc.main.cidr_block
}
```

**Using the module**:
```hcl
# main.tf
module "production_vpc" {
  source = "./modules/secure-vpc"
  
  vpc_cidr    = "10.0.0.0/16"
  environment = "production"
}

module "staging_vpc" {
  source = "./modules/secure-vpc"
  
  vpc_cidr    = "10.1.0.0/16"
  environment = "staging"
}

# Reference module outputs
output "prod_vpc_id" {
  value = module.production_vpc.vpc_id
}
```

**Module sources**: **Local modules**: `source = "./modules/vpc"` - modules in same repo. **Terraform Registry**: `source = "terraform-aws-modules/vpc/aws"` - public registry. **GitHub**: `source = "github.com/org/repo//modules/vpc"` - Git repos. **Version constraints**: `source = "terraform-aws-modules/vpc/aws"` with `version = "3.14.0"`.

**Security benefits of modules**: **Embedded security controls** - security best practices baked into modules (encryption, access controls, logging). **Consistent security** - all usage inherits security configurations. **Centralized updates** - fix security issues once in module, update everywhere. **Security review** - review module once, confident in all usage. **Golden modules** - organization-approved secure patterns.

**Example security-focused module**:
```hcl
# modules/secure-s3-bucket/main.tf
resource "aws_s3_bucket" "this" {
  bucket = var.bucket_name
}

# Force encryption
resource "aws_s3_bucket_server_side_encryption_configuration" "this" {
  bucket = aws_s3_bucket.this.id
  
  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm     = "aws:kms"
      kms_master_key_id = var.kms_key_id
    }
  }
}

# Block public access (always)
resource "aws_s3_bucket_public_access_block" "this" {
  bucket = aws_s3_bucket.this.id
  
  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# Enable versioning
resource "aws_s3_bucket_versioning" "this" {
  bucket = aws_s3_bucket.this.id
  
  versioning_configuration {
    status = "Enabled"
  }
}

# Logging
resource "aws_s3_bucket_logging" "this" {
  bucket = aws_s3_bucket.this.id
  
  target_bucket = var.logging_bucket
  target_prefix = "s3-logs/${var.bucket_name}/"
}
```

**Module best practices**: **Single responsibility** - module should do one thing well. **Well-defined interface** - clear inputs (variables) and outputs. **Documentation** - README explaining purpose, inputs, outputs, examples. **Versioning** - semantic versioning for registry modules. **Testing** - automated tests validating module functionality. **Security defaults** - secure by default, require opt-in for less secure configurations. **No hardcoded values** - parameterize everything that might vary.

### 10. What is the difference between a root module and a child module?

**Root module** is the main working directory where you run Terraform commands, containing the primary `.tf` files and calling other modules. **Child modules** are external modules called by the root module, typically stored in subdirectories or external sources.

**Root module characteristics**: **Entry point** - where `terraform init`, `terraform apply` run. **Configuration** - contains provider configuration, backend configuration. **Orchestration** - calls child modules, passes variables, uses outputs. **State** - state file created in root module context. **Variables** - can have variables passed via CLI, environment, or .tfvars files.

**Child module characteristics**: **Reusable components** - designed for reuse across projects. **Encapsulation** - internal resources hidden, only inputs/outputs exposed. **No provider config** - inherits providers from root module. **No backend** - no separate state, resources tracked in root module state. **Versioning** - can have versions (especially registry modules).

**Example structure**:
```
project-root/  # ROOT MODULE
├── main.tf
├── variables.tf
├── outputs.tf
├── terraform.tfvars
├── modules/  # CHILD MODULES
│   ├── vpc/
│   │   ├── main.tf
│   │   ├── variables.tf
│   │   └── outputs.tf
│   └── ec2/
│       ├── main.tf
│       ├── variables.tf
│       └── outputs.tf
└── .terraform/
```

**Root module (main.tf)**:
```hcl
terraform {
  required_version = ">= 1.0"
  
  backend "s3" {
    bucket = "terraform-state"
    key    = "prod/terraform.tfstate"
  }
}

provider "aws" {
  region = var.aws_region
}

# Calling child modules
module "vpc" {
  source = "./modules/vpc"
  
  vpc_cidr    = "10.0.0.0/16"
  environment = "production"
}

module "web_servers" {
  source = "./modules/ec2"
  
  vpc_id     = module.vpc.vpc_id  # Using child module output
  subnet_ids = module.vpc.private_subnet_ids
  count      = 3
}

# Root module variables
variable "aws_region" {
  default = "us-east-1"
}

# Root module outputs
output "vpc_id" {
  value = module.vpc.vpc_id
}
```

**Child module (modules/vpc/main.tf)**:
```hcl
# No provider or backend configuration (inherits from root)

variable "vpc_cidr" {
  description = "VPC CIDR block"
  type        = string
}

variable "environment" {
  description = "Environment name"
  type        = string
}

resource "aws_vpc" "main" {
  cidr_block = var.vpc_cidr
  
  tags = {
    Name = "${var.environment}-vpc"
  }
}

output "vpc_id" {
  value = aws_vpc.main.id
}

output "vpc_cidr" {
  value = aws_vpc.main.cidr_block
}
```

**Key differences**:
| Aspect | Root Module | Child Module |
|--------|-------------|--------------|
| Provider config | Required | Inherited |
| Backend config | Required | Not allowed |
| State file | Creates/manages | Resources tracked in root state |
| Execution | Direct (terraform apply) | Called by root |
| Variables | Multiple sources | Only from module call |
| Purpose | Orchestration | Reusable component |

**Data flow**: Root module → passes variables → Child module, Child module → returns outputs → Root module, Root module → orchestrates multiple child modules.

**Security implications**: **Root module security** - secures backend configuration (state encryption, locking), manages provider credentials, enforces policy-as-code, and controls what modules are called. **Child module security** - implements security best practices internally, validates input variables, and provides secure defaults. **Separation of concerns** - root module handles environment-specific config, child modules contain reusable secure patterns. **Version control** - child modules versioned independently enabling security patches, root module pins versions for stability.

**Best practices**: **Root module** - minimal code (mostly module calls), environment-specific values, backend and provider configuration, orchestration logic. **Child modules** - generic and reusable, well-documented interface, secure by default, no environment-specific hardcoding. **Communication** - explicit through variables (input) and outputs (return values), child modules self-contained. Understanding root vs. child modules is key to organizing Terraform code effectively and maintaining security boundaries.

### 11. What is the typical file structure of a Terraform module?

A well-organized Terraform module follows a standard file structure enabling readability, maintainability, and reusability.

**Basic module structure**:
```
module-name/
├── main.tf          # Primary resources
├── variables.tf     # Input variables
├── outputs.tf       # Output values
├── versions.tf      # Version constraints
├── README.md        # Documentation
├── .terraform.lock.hcl  # Dependency lock
├── examples/        # Usage examples
│   └── basic/
│       ├── main.tf
│       └── variables.tf
├── tests/           # Automated tests
│   └── module_test.go
└── .gitignore       # Git ignore patterns
```

**Core files explained**:

**`main.tf`** - primary resource definitions:
```hcl
resource "aws_instance" "this" {
  ami           = var.ami_id
  instance_type = var.instance_type
  
  # Security: encrypted EBS
  root_block_device {
    encrypted = true
  }
  
  tags = merge(var.tags, {
    Name = var.name
  })
}
```

**`variables.tf`** - input variable declarations:
```hcl
variable "ami_id" {
  description = "AMI ID for EC2 instance"
  type        = string
}

variable "instance_type" {
  description = "EC2 instance type"
  type        = string
  default     = "t3.micro"
  
  validation {
    condition     = contains(["t3.micro", "t3.small", "t3.medium"], var.instance_type)
    error_message = "Instance type must be t3.micro, t3.small, or t3.medium"
  }
}

variable "tags" {
  description = "Tags to apply to resources"
  type        = map(string)
  default     = {}
}
```

**`outputs.tf`** - output value declarations:
```hcl
output "instance_id" {
  description = "EC2 instance ID"
  value       = aws_instance.this.id
}

output "private_ip" {
  description = "Private IP address"
  value       = aws_instance.this.private_ip
}

output "public_ip" {
  description = "Public IP address"
  value       = aws_instance.this.public_ip
  sensitive   = true  # Mark sensitive data
}
```

**`versions.tf`** - Terraform and provider version constraints:
```hcl
terraform {
  required_version = ">= 1.0"
  
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = ">= 4.0, < 5.0"
    }
  }
}
```

**`README.md`** - comprehensive documentation:
```markdown
# Secure EC2 Module

## Description
Creates EC2 instances with security best practices enabled:
- Encrypted EBS volumes
- Instance profile with least privilege
- Security group with minimal access
- CloudWatch monitoring enabled

## Usage
```hcl
module "web_server" {
  source = "./modules/ec2"
  
  ami_id        = "ami-abc123"
  instance_type = "t3.micro"
  subnet_id     = "subnet-xyz"
  
  tags = {
    Environment = "production"
  }
}
```

## Inputs
| Name | Description | Type | Default | Required |
|------|-------------|------|---------|----------|
| ami_id | AMI ID | string | n/a | yes |
| instance_type | Instance type | string | t3.micro | no |

## Outputs
| Name | Description |
|------|-------------|
| instance_id | EC2 instance ID |

## Security Considerations
- EBS volumes encrypted by default
- No public IP assigned
- Runs in private subnet
```

**Larger modules** with multiple resource types:
```
complex-module/
├── main.tf              # Main orchestration
├── variables.tf         # All variables
├── outputs.tf           # All outputs
├── versions.tf          # Version constraints
├── ec2.tf               # EC2-specific resources
├── security-groups.tf   # Security group resources
├── iam.tf               # IAM resources
├── data.tf              # Data sources
├── locals.tf            # Local values
├── README.md            # Documentation
└── examples/
    ├── basic/
    └── advanced/
```

**Security-focused structure**:
```
secure-app-module/
├── main.tf              # Core resources
├── variables.tf         # Variables
├── outputs.tf           # Outputs
├── versions.tf          # Versions
├── security-groups.tf   # Network security
├── iam.tf               # Access control
├── kms.tf               # Encryption keys
├── monitoring.tf        # CloudWatch, alerts
├── compliance.tf        # Compliance resources
├── README.md            # Documentation
├── SECURITY.md          # Security documentation
└── examples/
    └── secure-deployment/
```

**`.gitignore`** for Terraform:
```
# .gitignore
.terraform/
*.tfstate
*.tfstate.*
.terraform.lock.hcl  # Or commit for consistency
*.tfvars  # Don't commit sensitive values
**/.terraform/*
crash.log
override.tf
override.tf.json
```

**Best practices for file organization**: **Separation of concerns** - each file has clear purpose. **Naming conventions** - descriptive file names (security-groups.tf, not sg.tf). **Size limits** - files shouldn't exceed 300-500 lines (split if larger). **Logical grouping** - related resources together. **README first** - always include comprehensive README. **Examples included** - show how to use the module. **Security documentation** - explain security decisions and configurations.

**Module sizing guidance**: **Small modules** (< 10 resources) - single main.tf is fine. **Medium modules** (10-50 resources) - split by resource type (ec2.tf, rds.tf, etc.). **Large modules** (> 50 resources) - consider breaking into sub-modules. A well-structured module is self-documenting, easy to understand, testable, and maintainable - essential for security-critical infrastructure code that will be reviewed and audited.

## Terraform – State, Backends, and Drift

### 12. What is the Terraform state file, and why is it important?

The Terraform state file (`terraform.tfstate`) is JSON file tracking the current state of managed infrastructure, mapping Terraform configuration to real-world resources.

**What state contains**: **Resource mapping** - links Terraform resource IDs to cloud provider resource IDs (terraform resource `aws_instance.web` → AWS instance `i-abc123`). **Resource attributes** - stores all resource attributes including computed values (IPs, IDs, ARNs). **Metadata** - version info, backend config, resource dependencies. **Outputs** - cached output values. **Provider configuration** - references to configured providers.

**Example state snippet**:
```json
{
  "version": 4,
  "terraform_version": "1.5.0",
  "resources": [
    {
      "mode": "managed",
      "type": "aws_instance",
      "name": "web",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "attributes": {
            "id": "i-abc123def456",
            "ami": "ami-0c55b159cbfafe1f0",
            "instance_type": "t3.micro",
            "private_ip": "10.0.1.50",
            "public_ip": "54.123.45.67"
          }
        }
      ]
    }
  ]
}
```

**Why state is crucial**: **Resource tracking** - Terraform knows what it manages vs. other resources, can update/destroy only managed resources. **Performance** - avoids querying cloud APIs for every operation, state provides cached resource info. **Collaboration** - shared state enables team collaboration, prevents conflicts through locking. **Drift detection** - comparing state to reality reveals configuration drift. **Dependency resolution** - state tracks resource dependencies enabling proper order of operations.

**State workflow**:
```
1. terraform plan
   - Read state file
   - Query current config
   - Compare state vs config
   - Generate execution plan

2. terraform apply
   - Execute plan
   - Update state with new resource info
   - Write updated state file

3. Next plan/apply cycle repeats
```

**Security implications - STATE FILES CONTAIN SENSITIVE DATA**:

**Sensitive data in state**: Passwords and secrets (database passwords, API keys), Private keys (SSH keys, TLS certificates), IP addresses and network topology, Resource IDs enabling targeted attacks, Configuration details revealing vulnerabilities.

**Example sensitive data in state**:
```json
{
  "resources": [
    {
      "type": "aws_db_instance",
      "instances": [{
        "attributes": {
          "password": "SuperSecretPassword123!",  // ⚠️ PLAINTEXT PASSWORD
          "username": "admin",
          "endpoint": "mydb.abc123.us-east-1.rds.amazonaws.com:3306"
        }
      }]
    }
  ]
}
```

**State security requirements**: **Never commit to Git** - `.gitignore` should include `*.tfstate`, accidental commits expose all secrets. **Encrypt at rest** - remote backends should encrypt state (S3 with KMS, Azure Storage with encryption). **Encrypt in transit** - HTTPS for remote state access. **Access control** - strict IAM/RBAC on state storage, only authorized users/systems access state. **Versioning** - enable versioning for backup/recovery (S3 versioning, Azure blob versioning). **Locking** - prevent concurrent modifications corrupting state.

**State file backup** - critical for disaster recovery:
```bash
# Manual backup before risky operations
terraform state pull > terraform.tfstate.backup

# S3 backend automatically versions
aws s3api list-object-versions \
  --bucket terraform-state \
  --prefix prod/terraform.tfstate
```

**Best practices**: **Remote state always** - never use local state in production. **Encryption mandatory** - state must be encrypted at rest and in transit. **Least privilege access** - limit who can read/write state. **State locking enabled** - prevent concurrent operations. **Regular backups** - automated state backups. **Audit logging** - track state access and modifications. **Separate states** - different state files per environment (dev/staging/prod). State file security is non-negotiable - a compromised state file is a complete security breach exposing all infrastructure secrets and topology.

### 13. Why is storing Terraform state remotely considered a best practice?

Remote state storage is essential for production Terraform usage, providing collaboration, security, and reliability benefits that local state cannot.

**Problems with local state**: **No collaboration** - only one person can work at a time, state on laptop not accessible to team. **No locking** - concurrent operations can corrupt state. **No backup** - losing laptop = losing state (disaster). **No encryption** - state sitting on disk potentially unencrypted. **No versioning** - can't recover from mistakes. **No audit trail** - no record of who changed what.

**Benefits of remote state**:

**1. Collaboration**: **Shared access** - entire team accesses same state, enables distributed team work. **Concurrent safety** - state locking prevents simultaneous operations. **Consistency** - everyone works from same source of truth.

**2. Security**: **Encryption at rest** - state encrypted in storage (S3 with KMS, Azure with encryption). **Encryption in transit** - HTTPS for state access. **Access control** - IAM/RBAC controls who can read/write state. **Audit logging** - track all state access (CloudTrail, Azure Monitor). **No local copies** - state never sits on developer laptops unencrypted.

**3. Reliability**: **Durability** - S3 11 9's durability, Azure/GCP equivalent. **Versioning** - automatic state versioning for rollback. **Backup** - built-in backup and recovery. **Disaster recovery** - state survives laptop failures, team member departures.

**4. CI/CD integration**: **Automated deployments** - CI/CD systems access remote state, no manual state management. **Consistent environments** - all environments use same state management pattern.

**Remote backend examples**:

**S3 backend** (most common for AWS):
```hcl
terraform {
  backend "s3" {
    bucket         = "terraform-state-prod"
    key            = "project/terraform.tfstate"
    region         = "us-east-1"
    
    # Security essentials
    encrypt        = true  # Encrypt state
    kms_key_id     = "arn:aws:kms:us-east-1:123456789:key/abc-123"
    
    # Locking
    dynamodb_table = "terraform-locks"
    
    # Versioning (enable on bucket)
  }
}
```

**Azure Storage backend**:
```hcl
terraform {
  backend "azurerm" {
    resource_group_name  = "terraform-state-rg"
    storage_account_name = "terraformstateprod"
    container_name       = "tfstate"
    key                  = "prod.terraform.tfstate"
  }
}
```

**Terraform Cloud backend**:
```hcl
terraform {
  backend "remote" {
    organization = "my-org"
    
    workspaces {
      name = "production"
    }
  }
}
```

**GCS backend** (Google Cloud Storage):
```hcl
terraform {
  backend "gcs" {
    bucket  = "terraform-state-prod"
    prefix  = "terraform/state"
  }
}
```

**Setting up secure S3 backend** (complete example):
```bash
# 1. Create S3 bucket
aws s3api create-bucket \
  --bucket terraform-state-prod \
  --region us-east-1

# 2. Enable versioning
aws s3api put-bucket-versioning \
  --bucket terraform-state-prod \
  --versioning-configuration Status=Enabled

# 3. Enable encryption
aws s3api put-bucket-encryption \
  --bucket terraform-state-prod \
  --server-side-encryption-configuration '{
    "Rules": [{
      "ApplyServerSideEncryptionByDefault": {
        "SSEAlgorithm": "aws:kms",
        "KMSMasterKeyID": "arn:aws:kms:..."
      },
      "BucketKeyEnabled": true
    }]
  }'

# 4. Block public access
aws s3api put-public-access-block \
  --bucket terraform-state-prod \
  --public-access-block-configuration \
    "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true"

# 5. Enable logging
aws s3api put-bucket-logging \
  --bucket terraform-state-prod \
  --bucket-logging-status file://logging.json

# 6. Create DynamoDB table for locking
aws dynamodb create-table \
  --table-name terraform-locks \
  --attribute-definitions AttributeName=LockID,AttributeType=S \
  --key-schema AttributeName=LockID,KeyType=HASH \
  --billing-mode PAY_PER_REQUEST
```

**IAM policy for state access** (least privilege):
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:ListBucket"
      ],
      "Resource": "arn:aws:s3:::terraform-state-prod"
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject"
      ],
      "Resource": "arn:aws:s3:::terraform-state-prod/*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "dynamodb:DescribeTable",
        "dynamodb:GetItem",
        "dynamodb:PutItem",
        "dynamodb:DeleteItem"
      ],
      "Resource": "arn:aws:dynamodb:*:*:table/terraform-locks"
    }
  ]
}
```

**Migration from local to remote** state:
```bash
# 1. Configure backend in Terraform config
# (add backend block to versions.tf)

# 2. Initialize (migrates state)
terraform init -migrate-state

# 3. Verify migration
terraform state list

# 4. Delete local state
rm terraform.tfstate*
```

**Best practices**: Use remote state for all non-trivial projects, enable encryption always, implement state locking, enable versioning for rollback, restrict access via IAM/RBAC, enable audit logging, separate state per environment, and regular state backups (though versioning provides this). Remote state is foundational to secure, collaborative Terraform usage - it's not optional for production workloads.

### 14. What are remote backends in Terraform?

Remote backends are storage locations for Terraform state files that are accessed over a network, providing collaboration, security, and reliability features.

**Backend types**:

**Standard backends** (state storage only): **S3** - AWS S3 bucket, most popular for AWS users. **Azure Storage** - Azure Blob Storage. **GCS** - Google Cloud Storage. **HTTP** - generic HTTP endpoint. **Consul** - HashiCorp Consul key-value store. **etcd** - etcd key-value store.

**Enhanced backends** (state + operations): **Terraform Cloud** - HashiCorp's SaaS offering with UI, RBAC, policies. **Terraform Enterprise** - self-hosted Terraform Cloud.

**Backend configuration syntax**:
```hcl
terraform {
  backend "backend_type" {
    # Configuration arguments
  }
}
```

**Common backends detailed**:

**S3 Backend**:
```hcl
terraform {
  backend "s3" {
    # Required
    bucket = "my-terraform-state"
    key    = "path/to/terraform.tfstate"
    region = "us-east-1"
    
    # Security
    encrypt        = true
    kms_key_id     = "arn:aws:kms:..."
    
    # Locking
    dynamodb_table = "terraform-locks"
    
    # Authentication
    # Uses AWS credentials chain (env vars, instance profile, etc.)
  }
}
```

**Azure Storage Backend**:
```hcl
terraform {
  backend "azurerm" {
    resource_group_name  = "tfstate-rg"
    storage_account_name = "tfstate"
    container_name       = "tfstate"
    key                  = "terraform.tfstate"
    
    # Uses Azure authentication (az cli, service principal, managed identity)
  }
}
```

**GCS Backend**:
```hcl
terraform {
  backend "gcs" {
    bucket  = "tf-state-bucket"
    prefix  = "terraform/state"
    
    # Uses GCP authentication (gcloud, service account)
  }
}
```

**Terraform Cloud Backend**:
```hcl
terraform {
  cloud {
    organization = "my-org"
    
    workspaces {
      name = "production"
    }
  }
}
```

**Backend features comparison**:
| Feature | S3 | Azure Storage | GCS | Terraform Cloud |
|---------|----|--------------|----|----------------|
| State storage | ✓ | ✓ | ✓ | ✓ |
| State locking | ✓ (DynamoDB) | ✓ (built-in) | ✓ (built-in) | ✓ |
| Encryption | ✓ (KMS) | ✓ | ✓ (CMEK) | ✓ |
| Versioning | ✓ (S3 versioning) | ✓ (blob versioning) | ✓ (object versioning) | ✓ |
| UI | ✗ | ✗ | ✗ | ✓ |
| RBAC | IAM | Azure RBAC | IAM | ✓ |
| Policy as Code | ✗ | ✗ | ✗ | ✓ (Sentinel) |
| Cost | $$ (S3 + DynamoDB) | $ | $$ | Free tier, then $$$ |

**Backend initialization**:
```bash
# Initialize backend
terraform init

# Migrate from local to remote
terraform init -migrate-state

# Reconfigure backend (change config)
terraform init -reconfigure

# View current backend config
terraform version
```

**Backend configuration methods**:

**Method 1: In configuration** (recommended):
```hcl
terraform {
  backend "s3" {
    bucket = "terraform-state"
    key    = "prod/terraform.tfstate"
  }
}
```

**Method 2: Partial configuration + CLI**:
```hcl
# versions.tf
terraform {
  backend "s3" {}  # Partial config
}
```
```bash
# Provide rest via CLI
terraform init \
  -backend-config="bucket=terraform-state" \
  -backend-config="key=prod/terraform.tfstate"
```

**Method 3: Backend config file**:
```hcl
# backend.hcl
bucket = "terraform-state"
key    = "prod/terraform.tfstate"
region = "us-east-1"
encrypt = true
```
```bash
terraform init -backend-config=backend.hcl
```

**Security considerations for backends**:

**Access control**: Strict IAM/RBAC permissions, separate read vs. write permissions, CI/CD service accounts with minimal permissions, human users with appropriate access.

**Encryption**: Always enable encryption at rest, use customer-managed keys when possible (KMS, CMEK), encryption in transit (HTTPS).

**Audit logging**: Enable access logs (CloudTrail, Azure Monitor, Cloud Audit Logs), monitor for unauthorized access, alert on state modifications.

**Network security**: Private endpoints/VPC endpoints where possible, no public internet access in production, firewall rules restricting access.

**Best practices**: **Never store backend config with secrets in Git** - use partial config + CLI/env vars. **Separate backends per environment** - different buckets for dev/staging/prod. **Enable versioning** - recover from mistakes. **Regular backups** - automated backup beyond versioning. **Monitoring** - alert on state access patterns. **Documentation** - document backend setup for team. Choosing right backend depends on cloud provider, team size, security requirements, and budget - but any remote backend is better than local state for production use.

### 15. What is state locking and why is it important?

State locking prevents concurrent Terraform operations that could corrupt the state file by ensuring only one operation modifies state at a time.

**Why locking is critical**: **Prevent corruption** - two simultaneous `terraform apply` commands would race to update state, resulting in corrupted state, lost updates, or broken infrastructure. **Ensure consistency** - guarantees state file represents single consistent point in time. **Enable collaboration** - allows team members to work safely without coordination overhead. **Protect against mistakes** - prevents accidental simultaneous deployments.

**How locking works**:
```
1. terraform apply starts
2. Acquire lock on state
   - If lock exists: wait or fail
   - If no lock: acquire and proceed
3. Perform operations
4. Update state
5. Release lock
```

**Lock acquisition example**:
```bash
# Terminal 1
$ terraform apply
Acquiring state lock. This may take a few moments...

# Terminal 2 (simultaneous)
$ terraform apply
Acquiring state lock. This may take a few moments...
Error: Error acquiring the state lock

Error message: ConditionalCheckFailedException: The conditional request failed
Lock Info:
  ID:        a1b2c3d4-5678-90ab-cdef-1234567890ab
  Path:      terraform-state-prod/prod.tfstate
  Operation: OperationTypeApply
  Who:       alice@workstation
  Version:   1.5.0
  Created:   2024-01-20 10:30:15 UTC
  Info:

Terraform acquires a state lock to protect the state from being written
by multiple users at the same time. Please resolve the issue above and try
again.
```

**Locking mechanisms by backend**:

**S3 + DynamoDB** (AWS):
```hcl
terraform {
  backend "s3" {
    bucket         = "terraform-state"
    key            = "terraform.tfstate"
    region         = "us-east-1"
    dynamodb_table = "terraform-locks"  # Locking table
    encrypt        = true
  }
}
```

DynamoDB table structure:
```bash
# Create locking table
aws dynamodb create-table \
  --table-name terraform-locks \
  --attribute-definitions AttributeName=LockID,AttributeType=S \
  --key-schema AttributeName=LockID,KeyType=HASH \
  --billing-mode PAY_PER_REQUEST

# Lock entry format
{
  "LockID": "bucket-name/path/terraform.tfstate-md5",
  "Info": {
    "ID": "unique-lock-id",
    "Operation": "OperationTypeApply",
    "Who": "user@host",
    "Version": "1.5.0",
    "Created": "2024-01-20T10:30:15Z"
  }
}
```

**Azure Storage** - built-in blob lease locking:
```hcl
terraform {
  backend "azurerm" {
    resource_group_name  = "tfstate-rg"
    storage_account_name = "tfstate"
    container_name       = "tfstate"
    key                  = "terraform.tfstate"
    # Locking automatic via blob leases
  }
}
```

**GCS** - built-in locking via object metadata:
```hcl
terraform {
  backend "gcs" {
    bucket  = "tf-state-bucket"
    prefix  = "terraform/state"
    # Locking automatic
  }
}
```

**Terraform Cloud** - built-in locking with UI:
```hcl
terraform {
  cloud {
    organization = "my-org"
    workspaces {
      name = "production"
    }
  }
}
```

**Force unlock** (use with extreme caution):
```bash
# If lock stuck (crashed process, etc.)
terraform